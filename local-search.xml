<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>超参数调整(HP Tuning)与激活函数</title>
    <link href="/2023/01/27/Code/DL/%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/"/>
    <url>/2023/01/27/Code/DL/%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</url>
    
    <content type="html"><![CDATA[<h1id="hphyperparameters调试tuning">HP(Hyperparameters)调试(Tuning)</h1><table><thead><tr class="header"><th style="text-align: center;">超参数</th><th style="text-align: center;">取值</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><strong>学习速率<spanclass="math inline">\(α\)</span></strong></td><td style="text-align: center;">需调整</td></tr><tr class="even"><td style="text-align: center;">Momentum：<spanclass="math inline">\(β\)</span></td><td style="text-align: center;">$ 0.9$</td></tr><tr class="odd"><td style="text-align: center;">Adam：<spanclass="math inline">\(β_1\)</span></td><td style="text-align: center;">$ 0.9 $</td></tr><tr class="even"><td style="text-align: center;">Adam：<spanclass="math inline">\(β_2\)</span></td><td style="text-align: center;"><spanclass="math inline">\(0.999\)</span></td></tr><tr class="odd"><td style="text-align: center;">Adam：<spanclass="math inline">\(ε\)</span></td><td style="text-align: center;">$10^{−8} $</td></tr><tr class="even"><td style="text-align: center;">#layers</td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">learning_rate decay</td><td style="text-align: center;"></td></tr><tr class="even"><td style="text-align: center;">#hidden unit</td><td style="text-align: center;"></td></tr><tr class="odd"><td style="text-align: center;">#mini-batch size</td><td style="text-align: center;"></td></tr></tbody></table><p>重要性 ： <span class="math inline">\(α\)</span>最为重要 其次为 <spanclass="math inline">\(β\)</span>，#mini-batch size ,#hidden unit...</p><p>参数选择有以下一些方法:</p><ol type="1"><li><p>随机选择点。例如现在有 α与 Adam 的 ε两个超参数要调试。在参数取值范围内随机选择若干点，可以发现哪个超参数更重要，影响更大.</p></li><li><p>由粗糙到精细的策略。由1，发现在某个点效果最好，可以预测在该点附近效果也很好，于是放大这块区域,更密集地取值. <img src="/img/imgDL/search.png" /></p></li><li><p>随机选择点时，有些参数不适合均匀(在线性轴上)的随机选择,就要选择适当的尺度.</p><p>例如 α，我们希望其在对数轴上随机取点(0.0001, 0.001, 0.01, 0.1,1)，我们可以<code>a= 10** (-4*np.random.rand())</code>，即可得到 <spanclass="math inline">\(a∈[10^{−4},10^0]\)</span>,相比对数尺度之下线性均匀尺度效率就十分低下。</p><p>类似可对β取点，比如<span class="math inline">\(β=0.9...0.999,1−β=0.1...0.001， 1−β∈[10^{−3},10^{−1}]\)</span></p></li></ol><h4id="两种调参思路-babysitting-training-many-models-in-parallel">两种调参思路:babysitting &amp; Training many models in parallel</h4><p><img src="/img/imgDL/2way.png" /></p><p>关键取决于你的计算资源</p><h3 id="batch-归一化batch-normalization---bn">2.Batch 归一化(BatchNormalization)---BN</h3><p>会使你的参数搜索问题变的容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，学习算法运行速度更快</p><h4 id="实现">2.1 实现</h4><blockquote><p><strong>一般框架都有，直接调用即可.</strong></p></blockquote><p>训练 Logistic 回归时, 归一化<spanclass="math inline">\(X\)</span>可以加快学习过程.现在我们希望对隐藏层的<span class="math inline">\(Z\)</span>归一化.(或者<span class="math inline">\(A\)</span>)</p><p>对每一层的<span class="math inline">\(z\ or\a\)</span>(都可，一般选z)做如下操作:</p><p><span class="math display">\[平均值:\mu = \frac{1}{m} \sum_i z^{(i)} ,\\方差:\sigma^2 = \frac{1}{m} \sum_i (z^{(i)} - \mu)^2 \\\]</span> <span class="math display">\[z_{\text{norm}}^{(i)} = \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 +\varepsilon}} \\\]</span> <span class="math display">\[\widetilde{z}^{(i)} = \gamma z_{\text{norm}}^{(i)} + \beta\]</span></p><p>(ε是为了防止分母为0.)</p><ul><li><p><span class="math inline">\(z_{norm}\)</span>就是标准化的 <spanclass="math inline">\(z\)</span>,<spanclass="math inline">\(z\)</span>的每一个分量都含有,平均值为0,方差为1。</p></li><li><p>不想让隐藏单元总是含有平均值0和方差1（也许隐藏单元有了不同的分布会有意义）,计算 <span class="math inline">\(\widetilde{z}^{(i)}\)</span>.</p></li><li><p>γ与 β是模型的学习参数, 梯度下降时会像更新神经网络的权重一样更新γ和 β。γ与 β的作用是：可以随意设置 z˜(i)的平均值。 当 $ γ=$ 且 $ β=μ $时,<span class="math inline">\(z˜(i)=z(i)\)</span>,通过赋予γ和β其他值，可以使你构造含其他平均值和方差的隐藏单元值。</p></li><li><p><strong>Batch归一化的作用</strong>：是它适合的归一化过程不只是输入层，同样适用于神经网络中的深度隐藏层。Batch归一化了一些隐藏单元值中的平均值和方差。</p></li><li><p>BN算法有一定的正则化效果，一般batch_size=64,128,256,越大，则正则化效果越弱,但是切莫将其视作正则化方法.</p></li><li><p>实际算法中一般不用线性AVG,而用计算<spanclass="math inline">\(\mu,\sigma\)</span>使用指数加权平均EMA(当然具体要看框架手册说明).</p></li></ul><h3 id="激活函数">激活函数</h3><blockquote><ol type="1"><li>Sigmoid(z)<br /><span class="math display">\[g(z)=\frac1{1+e^{-z}}\]</span>(注意：来自sigmoid 函数的输出值可以很容易地理解为概率。)这个一般用不到，但是二元回归时比较好(比如区分是不是猫),其他情况性能比tanh差输出的值介于0 和 1 之间，这使其成为二元分类的一个非常好的选择。</li></ol><p>如果输出小于 0.5，则可以将其归类为 0，如果输出大于 0.5，则归类为 1。它也可以用 tanh 来完成，但是它不太方便，因为输出在-1 和 1 之间。</p></blockquote><blockquote><ol start="2" type="1"><li>tanh(z) <spanclass="math display">\[g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}=\frac{2}{1+e^{-2x}}-1\]</span></li></ol><p>我们可以发现Tanh 函数可以看作放大并平移的Logistic 函数，其值域是(−1,1)。Tanh与sigmoid的关系如下： <spanclass="math display">\[tanh(x)=2sigmoid(2x)-1\]</span></p><p>tanh激活函数通常比Sigmoid激活函数性能更好，因为其输出的平均值接近于零，因此在下一层，可以更好地集中数据。</p></blockquote><blockquote><p>注意：在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。</p></blockquote><blockquote><ol start="3" type="1"><li>ReLU(z)(推荐)<br /><span class="math display">\[g(z)=max(0,z)\]</span>一般不知道选什么选这个 (有一个对负半区进行修正的leakyRelu,通常表现更好但是没有在实践中运用)</li></ol></blockquote><p>ReLU函数的不足：</p><ul><li><p>当输入为负时，ReLU完全失效，在正向传播过程中，这不是问题。有些区域很敏感，有些则不敏感。但是在反向传播过程中，如果输入负数，则梯度将完全为零；</p></li><li><p>【DeadReLU问题】ReLU神经元在训练时比较容易“死亡”。在训练时，如果参数在一次不恰当的更新后，第一个隐藏层中的某个ReLU神经元在所有的训练数据上都不能被激活，那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远不能被激活。这种现象称为死亡ReLU问题，并且也有可能会发生在其他隐藏层。</p></li><li><p>不以零为中心：和 Sigmoid 激活函数类似，ReLU函数的输出不以零为中心，ReLU 函数的输出为 0或正数,给后一层的神经网络引入偏置偏移，会影响梯度下降的效率。</p></li></ul><p>其他激活函数详见<ahref="https://zhuanlan.zhihu.com/p/364620596">知乎帖子</a></p><h4 id="多类别分类----softmax-as-a-激活函数">多类别分类--- softmax as a激活函数</h4><p>实际是二元回归推广到C元回归,一般有Softmax和Hardmax,Softmax给出的是每个分类的概率. 而对应的 Hardmax 则是将最大的元素输出为 1,其余元素置 0.</p><p>类似 Logistic 回归, 但 Softmax 回归能识别多个分类. 因此 <spanclass="math inline">\(\hat y\)</span>是 C×1 维的向量,给出C个分类的概率,所有概率加起来应该为1.</p><p>在神经网络的最后一层, 我们像往常一样计算各层的线性部分, 当计算了z[L]=W[L]a[L−1]+b[L]之后, 使用 Softmax 激活函数.</p><p><span class="math display">\[a^{[L]}_i =\frac{e^{z^{[L]}_i}}{\sum_{j=1}^{4} e^{z^{[L]}_i}}\]</span></p><p>Softmax 分类中, 一般使用的损失函数及反向传播的导数是</p><p><span class="math display">\[L(\hat{y}, y) = -\sum_{j=1}^{n}y_j \text{log } \hat{y}_j \\\]</span></p><p><span class="math display">\[J(w^{[1]}, b^{[1]}, ...) = \frac{1}{m} \sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)}) \\\]</span></p><p><span class="math display">\[\frac{\partial J}{\partial z^{[L]}} = \hat{y} - y\]</span></p>]]></content>
    
    
    <categories>
      
      <category>DL</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Python虚拟环境--以tensorflow安装配置为例</title>
    <link href="/2023/01/27/Code/env/py_env/"/>
    <url>/2023/01/27/Code/env/py_env/</url>
    
    <content type="html"><![CDATA[<h1 id="python-tensorflow虚拟环境----基于anaconda">pythontensorflow虚拟环境 ---基于Anaconda</h1><p><strong>date:</strong> 2023/01/27</p><p>python的虚拟环境向来困惑了很多新手(包括我),但实际上用过的人都很清楚其好处:封闭性好易于管理,并且营造纯净的环境避免冲突。</p><h2 id="概念">概念</h2><p>(如果只是装环境 <a href="##-实践">请跳过</a>)</p><ol type="1"><li><p>python 版本</p><p>Python 版本指的是 Python 解析器本身的版本。由于 Python3 不能与Python2 兼容，导致一些软件库需要设配两种版本的Python，同时开发者可能需要在一个环境中，部署不同版本的Python，对开发和维护造成了麻烦。</p><p>因此出现了版本管理器 Pyenv，类似于 nodejs 的nvm，可以创建出相互隔离的 Python 环境，并且可以方便的切换环境中的 Python版本，但和 Python 虚拟环境关系不大</p></li><li><p>python 包库</p><p>包库或者叫软件源是 Python第三方软件的库的集合，或者市场，可以发布、下载和管理软件包，其中 pypi是官方指定的软件包库，基于其上的 pip工具就是从这里查找、下载安装软件包的。为了提高下载速度，世界上有很多Pypi 的镜像服务器，在国内也有多个软件源.</p></li><li><p>python 包管理器</p><p>软件包源中的软件包数量巨大，版本多样，所以需要借助于软件源管理工具，例如pip、conda、Pipenv、Poetry 等</p><ul><li><p>pip 是最常用的包管理工具，通过<code>pip install &lt;packagename&gt;</code>命令格式来安装软件包，使用的是pypi 软件包源</p></li><li><p>conda 多用作科学计算领域的包管理工具，功能丰富且强大.</p></li><li><p>ipenv会自动帮你管理虚拟环境和依赖文件，并且提供了一系列命令和选项来帮助你实现各种依赖和环境管理相关的操作</p></li><li><p>Poetry 和 Pipenv 类似，是一个 Python虚拟环境和依赖管理工具，另外它还提供了包管理功能，比如打包和发布。你可以把它看做是Pipenv 和 Flit 这些工具的超集。它可以让你用 Poetry 来同时管理 Python库和 Python 程序</p></li></ul><p>很多包管理工具不仅提供了基本的包管理功能，还提供了虚拟环境构建，程序管理的等功能</p></li><li><p>Python 虚拟环境</p><p>Python应用经常需要使用一些包第三方包或者模块，有时需要依赖<strong>特定的包</strong>或者库的<strong>版本</strong>，所以不能有一个能适应所有Python 应用的软件环境，解决这一问题的方法是 虚拟环境。</p><p><strong>原理：</strong></p><p><div class="note note-success">            <p>操作系统的环境变量可以为程序提供信息和做信息交换介质，进程可以共享操作系统中的环境变量，也可以为进程指定环境变量，其中PATH是很重要的环境变量，用于为操作系统和程序提供可执行文件的访问路径.</p>          </div></p><p>Python虚拟环境就是利用这个特性构建的，在激活虚拟环境之时，激活脚本会将当前命令行程序的PATH 修改为虚拟环境的，这样执行命令就会在被修改的 PATH中查找，从而避免了原本 PATH 可以找到的命令，从而实现了 Python环境的隔离。</p></li></ol><h4 id="与开发工具配合">与开发工具配合</h4><ul><li><p>Visual Studio Code</p></li><li><p>Pycharm ## 实践</p></li></ul><p>最近在学习DL时正好又要搭建一个tensorflow的环境，因此在此用这篇文章回顾自己基于conda包安装tensorflow环境的过程。</p><p>环境： - windows 11 shell</p><ul><li>anaconda prompt3(默认已安装好anaconda并添加入env-path)</li></ul><blockquote><p>题外话 : chanel 记得换镜像源</p></blockquote><p>在shell中操作如下: <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">查看源</span><br>conda config --show-sources<br><span class="hljs-meta prompt_"># </span><span class="language-bash">建议使用科大源.下面图片是清华源的例子，因为清华源貌似也挺慢的所以换科大源了</span><br>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/main/<br>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/pkgs/free/<br>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/conda-forge/<br>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/msys2/<br>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/bioconda/<br>conda config --add channels https://mirrors.ustc.edu.cn/anaconda/cloud/menpo/<br>conda config --set show_channel_urls yes<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">设置搜索时显示通道地址</span><br>conda config --set show_channel_urls yes<br></code></pre></td></tr></table></figure></p><p>命令执行完后，会生成~/.condarc(Linux/Mac)或C:_NAME.condarc文件,这便是配置文件，执行后应该长这样(像上面的改配置也可以直接修改这个文件):</p><p><img src="/img/py_env/config_chanel.png" />要把default删除或者移到第一个，因为pip是按这个列表从下往上依次执行,没找到相应的文件则从上一个找.</p><h2id="以创建tensorflow的安装与卸载环境为例">以创建tensorflow的安装与卸载环境为例</h2><p>anacondaprompt3打开会是一个叫base的基环境，也就是最基本的环境,python支持你自行创建多个虚拟环境以应对不同的开发,这里是以安装一个tensorflow的环境为例.</p><p>1.创建一个tensorflow环境创建一个环境用于安装tensorflow，环境名可以自己命名，选择python的版本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">conda create -n 环境的名字 python=版本号</span><br>conda create -n tensorflow2 python=3.7<br></code></pre></td></tr></table></figure><p>2.激活tensorflow环境</p><p><code>activate tensorflow2</code></p><p>注:退出虚拟环境<code>deactivate tensorflow2</code>即可。</p><p>3.查看当前可以使用的tensorflow版本 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">conda search  --full -name tensorflow<span class="hljs-comment">#下面这条运行不了就运行这条，是版本问题</span></span><br>conda search  --full tensorflow<br></code></pre></td></tr></table></figure></p><ol start="4" type="1"><li>查看tensorflow包信息及依赖关系</li></ol><p><code>conda  info  tensorflow</code></p><ol start="5" type="1"><li><p>安装tensorflow</p><p>在自己创建的环境下安装tensorflow</p></li></ol><p><code>pip install tensorflow</code>或<code>pip install --upgrade --ignore-installed      tensorflow</code>如果速度很慢就换用科大源(亲测源很快，妮可还是妮可) <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install tensorflow-cpu==2.2.0 -i  https://pypi.mirrors.ustc.edu.cn/simple  <br></code></pre></td></tr></table></figure></p><p>安装完之后 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">activate tensorflow2<br><span class="hljs-meta prompt_">#</span><span class="language-bash">...</span><br><span class="hljs-meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; import tensorflow as tf</span><br></code></pre></td></tr></table></figure></p><p>---没报错请跳过这段--- <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">TypeError: Descriptors cannot not be created directly.<br>If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc &gt;= 3.19.0.<br>If you cannot immediately regenerate your protos, some other possible workarounds are:<br> 1. Downgrade the protobuf package to 3.20.x or lower.<br> 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).<br></code></pre></td></tr></table></figure> 是这个protobuf包等级太高了,于是<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"> <span class="hljs-comment"># 卸载当前版本（我的是3.9.2）</span><br><span class="hljs-attribute">pip</span> uninstall protobuf<br><span class="hljs-comment"># 安装低版本</span><br><span class="hljs-attribute">pip</span> install protobuf==<span class="hljs-number">3</span>.<span class="hljs-number">20</span>.<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure> 然后就行了.</p><hr /><p>如图所示则已安装成功.bless you! <img src="/img/py_env/tf_env.png" />然后exit()或者Ctrl+z退出.</p><blockquote><p>注意: 这个虚拟环境在C:\Users\anaconda3\envs目录下</p></blockquote><h3id="下面看看在vscode上的jupyter-notebook运行.">下面看看在<strong>VSCode</strong>上的jupyternotebook运行.</h3><p>VSCode貌似没有自动找到我的解释器路径,于是ctrl+shift+p设置里输入<code>select python interpreter</code>,手动找到你的环境下的python比如：<code>C:\Users\username\anaconda3\envs\tensorflow2\python.exe</code></p><p>然后重启vscode或者设置里面输入<code>reload window</code></p><p>现在可以修改jupyter book的kernel了!然鹅运行还不行 <imgsrc="/img/py_env/ero2.png" /></p><p>按他说的做.打开终端运行</p><p><code>conda install -n base_tf ipykernel --update-deps --force-reinstall</code></p><p><img src="/img/py_env/terminal.png" /></p><p>然后一路yes,就能正常运行了</p><p><img src="/img/py_env/suceess.png" /></p>]]></content>
    
    
    <categories>
      
      <category>环境</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VSCode</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Optimization in DL</title>
    <link href="/2023/01/26/Code/DL/Notes5/"/>
    <url>/2023/01/26/Code/DL/Notes5/</url>
    
    <content type="html"><![CDATA[<h2 id="优化方法c2.u2">优化方法(C2.U2)</h2><h3 id="梯度优化算法">1. 梯度优化算法</h3><h4 id="mini-batch-梯度下降">1.1 Mini-batch 梯度下降</h4><p>将 <spanclass="math inline">\(X=[x(1),x(2),x(3),...,x(m)]\)</span>矩阵所有m个样本划分为t个子训练集,每个子训练集，也叫做mini-batch；每个子训练集称为 <spanclass="math inline">\(x^{\{i\}}\)</span>,每个子训练集内样本个数均相同(若每个子训练集有1000个样本, 则 <spanclass="math inline">\(x^{\{i\}}=[x(1),x(2),...,x(1000)]\)</span>,维度为(nx,1000).</p><p>例：把x(1)到x(1000)称为 X{1}, 把x(1001)到x(2000)称为X{2}，如果你的训练样本一共有500万个，每个mini-batch都有1000个样本，也就是说，你有5000个mini-batch,因为5000*1000=500万， 最后得到的是 X{5000}.</p><p>若m不能被子训练集样本数整除,则最后一个子训练集样本可以小于其他子训练集样本数。 Y亦然.</p><p>训练时, 每次迭代仅对一个子训练集（mini-batch）进行梯度下降:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs py">On iteration t:<br>    Repeat:For i=<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,...,t:<br>    Forward Prop On X&#123;i&#125;<br>    Compute Cost J&#123;i&#125;<br>    Back Prop using X&#123;i&#125;,Y&#123;i&#125;<br>    Update w,b<br></code></pre></td></tr></table></figure> 1.使用 batch 梯度下降法时：</p><p>一次遍历训练集只能让你做一个梯度下降；每次迭代都遍历整个训练集,预期每次迭代成本都会下降</p><p>2.但若使用 mini-batch 梯度下降法</p><p>一次遍历训练集，能让你做5000个梯度下降；如果想多次遍历训练集，你还需要另外设置一个while循环...</p><p>若对成本函数作图, 并不是每次迭代都下降, 噪声较大,但整体上走势还是朝下的.特别的当minibatch的size很小，几乎是随机梯度下降，噪声较大，最后甚至不会收敛于最小值，但一般会在附近摆动.</p><blockquote><p>1)若样本集较小(小于2000), 无需使用 mini-batch;</p><p>2)一般的 mini-batch 大小为 64~512, 通常为 2的整数次方(这样代码运行可能更快).</p></blockquote><p>一般的mini-batch GD还会加上Stochastic GradientDescent(随机梯度下降的方法，即先随机打乱样本再训练)</p><h4 id="code">Code</h4><p>注意：实现SGD总共需要3个for循环：</p><ol type="1"><li><p>迭代次数</p></li><li><p>m个训练数据</p></li><li><p>各层上(要更新所有参数，从(W1,b1)到(Wl,bl))</p></li></ol><p>实际上，如果你既不使用整个训练集也不使用一个训练示例来执行每次更新，则通常会得到更快的结果。小批量梯度下降法在每个步骤中使用中间数量的示例。通过小批量梯度下降，你可以遍历小批量，而不是遍历各个训练示例。</p><ul><li>Mini-Batch 梯度下降+ SGD</li></ul><p>让我们学习如何从训练集（X，Y）中构建小批次数据。</p><p>分两个步骤：</p><ol type="1"><li><p>Shuffle:如下所示，创建训练集（X，Y）的随机打乱版本。X和Y中的每一列代表一个训练示例。</p></li><li><p>Partition:将打乱后的（X，Y）划分为大小为mini_batch_size（此处为64）的小批处理。</p></li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_mini_batches</span>(<span class="hljs-params">X, Y, mini_batch_size = <span class="hljs-number">64</span>, seed = <span class="hljs-number">0</span></span>):<br>    <br>    np.random.seed(seed)            <span class="hljs-comment"># To make your &quot;random&quot; minibatches the same as ours</span><br>    m = X.shape[<span class="hljs-number">1</span>]                  <span class="hljs-comment"># number of training examples</span><br>    mini_batches = []<br>        <br>    <span class="hljs-comment"># Step 1: Shuffle (X, Y)</span><br>    permutation = <span class="hljs-built_in">list</span>(np.random.permutation(m))<br>    shuffled_X = X[:, permutation]<br>    shuffled_Y = Y[:, permutation].reshape((<span class="hljs-number">1</span>,m))<br><br>    <span class="hljs-comment"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span><br>    num_complete_minibatches = math.floor(m/mini_batch_size) <span class="hljs-comment"># number of mini batches of size mini_batch_size in your partitionning</span><br>    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_complete_minibatches):<br>        <span class="hljs-comment">### START CODE HERE ### (approx. 2 lines)</span><br>        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+<span class="hljs-number">1</span>) * mini_batch_size]<br>        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+<span class="hljs-number">1</span>) * mini_batch_size]<br>        mini_batch = (mini_batch_X, mini_batch_Y)<br>        mini_batches.append(mini_batch)<br><br>    <span class="hljs-comment"># Handling the end case (last mini-batch &lt; mini_batch_size)</span><br>    <span class="hljs-keyword">if</span> m % mini_batch_size != <span class="hljs-number">0</span>:<br>        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]<br>        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]<br>        mini_batch = (mini_batch_X, mini_batch_Y)<br>        mini_batches.append(mini_batch)<br><br>    <span class="hljs-keyword">return</span> mini_batches<br><br></code></pre></td></tr></table></figure><h4 id="指数加权平均数exponentially-weighted-averages">1.2指数加权平均数(Exponentially Weighted Averages)</h4><p><a href="https://zhuanlan.zhihu.com/p/68748778">参考博客</a></p><p>这个不是优化算法，是下面的优化方法的数学基础.</p><blockquote><p>指数移动平均EMA（Exponential MovingAverage）也叫权重移动平均（Weighted MovingAverage），是一种给予近期数据更高权重的平均方法。</p></blockquote><blockquote><ul><li>普通平均数<span class="math inline">\(\overline{v}=\frac1n\sum^n_{i=1}\theta_i\)</span></li></ul></blockquote><blockquote><ul><li>EMA :<span class="math inline">\(v_t =\betav_{t-1}+(1=\beta)\theta_t\)</span>，</li></ul></blockquote><blockquote><p><span class="math inline">\(v_t\)</span>表示前t条的平均值(<spanclass="math inline">\(v_0=0\)</span>)，<spanclass="math inline">\(\beta\)</span>是加权权重值(一般设为0.9-0.999)。</p></blockquote><figure><img src="/img/imgDL/exp_avg.png" alt="eg" /><figcaption aria-hidden="true">eg</figcaption></figure><p><span class="math display">\[v_t=βv_t−1+(1−β)θ_t,β∈[0,1)\]</span></p><p>Andrew Ng在Course 2 Improving Deep NeuralNetworks中讲到，EMA可以近似看成过去<spanclass="math inline">\(\frac{1}{1-\beta}\)</span>个时刻v值的平均。</p><p>普通的过去 n时刻的平均是这样的：<spanclass="math inline">\(v_t=\frac{(n-1)v_{t-1}+\theta_t}{n}\)</span>,</p><p>类比EMA，可以发现当 <span class="math inline">\(\beta=\frac{n-1}{n}\)</span>时，两式形式上相等。需要注意的是，两个平均并不是严格相等的，这里只是为了帮助理解。</p><p>实际上，EMA计算时，过去<spanclass="math inline">\(\frac{1}{1-\beta}\)</span>个时刻之前的数值平均会decay到<spanclass="math inline">\(\frac{1}{e}\)</span>的加权比例，证明如下。</p><p>如果将这里的<spanclass="math inline">\(v_t\)</span>展开，可以得到：</p><p><span class="math display">\[v_t=\alpha^n v_{t-n}+(1-\alpha)\left(\alpha^{n-1}\theta_{t-n+1}+\ldots+\alpha^0 \theta_t\right)\]</span> 其中,<spanclass="math inline">\(n=\frac{1}{1-\beta}\)</span>带入可得<spanclass="math inline">\(\alpha ^n=\alpha ^{\frac{1}{1-\alpha}}=\frac{1}{e}\)</span></p><p><strong>EMA的偏差修正</strong></p><p>实际使用中，如果令 <spanclass="math inline">\(v_0=0\)</span>，且步数较少，EMA的计算结果会有一定偏差。因此可以加一个偏差修正（biascorrection）： <span class="math display">\[v_t=\frac{v_t}{1+\beta^t}\]</span> 显然，当t很大时，修正近似于1。</p><h4 id="动量梯度下降法gradient-descent-with-momentum">1.3动量梯度下降法(Gradient Descent With Momentum)</h4><p><img src="/img/imgDL/momomt.png" /></p><p>真实训练可能会出现这样的情况:</p><ol type="1"><li><p>摆动向最小值前进，效率低</p></li><li><p>如果学习率较大直接超调(跳出凸区域)只能使用小学习率</p></li></ol><p>动量梯度下降法(Momentum)使用指数加权平均数EMA(计算梯度的指数加权平均数,并用该梯度更新你的权重,于是更平滑，可以用更大的学习率):<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># β一般为0.9</span><br><span class="hljs-keyword">for</span> iteration t:<br>    <span class="hljs-comment">##... compute dW,db</span><br>    v_dW=β*v_dW+(<span class="hljs-number">1</span>−β)*dW,<br>    v_db=β*v_db+(<span class="hljs-number">1</span>−β)*db<br>    W=W−α*v_dW<br>    b=b−α*v_db<br></code></pre></td></tr></table></figure></p><h4 id="rmsproproot-mean-square-prop">1.4 RMSprop(Root Mean Squareprop)</h4><p><img src="/img/imgDL/RMP.png" /></p><p>全称是均方根传递，能够很好的消除两方向不协调摆动，从而可以提高学习率.<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment"># β参数与EMA方法不同，一般为0.999</span><br><span class="hljs-keyword">for</span> iteration t:<br>    <span class="hljs-comment">##... compute dW,db</span><br>    SdW=β<span class="hljs-number">2</span>*SdW+(<span class="hljs-number">1</span>−β<span class="hljs-number">2</span>)*(dW)^<span class="hljs-number">2</span><br>    Sdb=β<span class="hljs-number">2</span>*Sdb+(<span class="hljs-number">1</span>−β<span class="hljs-number">2</span>)*(db)^<span class="hljs-number">2</span><br>    W=W−α*dW/sqrt(SdW)+epsilon  <span class="hljs-comment"># 分母+epsilon是为了数值稳定性，通常取10^-8</span><br>    b=b−α*db/sqrt(Sdb)+epsilon<br></code></pre></td></tr></table></figure></p><h4 id="adam-优化算法adaptive-moment-estimation">1.5 Adam优化算法(Adaptive Moment Estimation)</h4><p>RMSprop 与 Adam 是少有的经受住人们考验的两种算法. Adam 的本质就是将Momentum 和 RMSprop 结合在一起. 使用该算法首先需要初始化: <spanclass="math display">\[ v_{dW} = 0, S_{dW} = 0, v_{db} = 0, S_{db} =0.\]</span> 在第t次迭代中: <span class="math display">\[\begin{aligned}v_{dW}  = \beta_1 v_{dW} + (1 - \beta_1)dW ,v_{db}  = \beta_1 v_{db} + (1 - \beta_1)db \\S_{dW}  = \beta_2 S_{dW} + (1 - \beta_2)(dW)^2 ,\   S_{db}  = \beta_2 S_{db} + (1 - \beta_2)(db)^2 \\v_{dW}^{\text{corrected}}  = \frac{v_{dW}}{1-\beta_1^t}, \quadv_{db}^{\text{corrected}} = \frac{v_{db}}{1-\beta_1^t} \\S_{dW}^{\text{corrected}}  = \frac{S_{dW}}{1-\beta_2^t}, \quadS_{db}^{\text{corrected}} = \frac{S_{db}}{1-\beta_2^t} \\W  = W -\alpha\frac{v_{dW}^{\text{corrected}}}{\sqrt{S_{dW}^{\text{corrected}}+\varepsilon}},b  = b -\alpha\frac{v_{db}^{\text{corrected}}}{\sqrt{S_{db}^{\text{corrected}}+\varepsilon}}\end{aligned}\]</span></p><table><thead><tr class="header"><th>超参数</th><th>值</th></tr></thead><tbody><tr class="odd"><td><span class="math inline">\(β\)</span></td><td><span class="math inline">\(0.9\)</span></td></tr><tr class="even"><td><span class="math inline">\(β_2\)</span></td><td><span class="math inline">\(0.999\)</span></td></tr><tr class="odd"><td><span class="math inline">\(\epsilon\)</span></td><td><span class="math inline">\(10^{-8}\)</span></td></tr><tr class="even"><td><span class="math inline">\(\alpha\)</span></td><td>调整</td></tr></tbody></table><h4 id="code-1">Code</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">initialize_adam</span>(<span class="hljs-params">parameters</span>) :<br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span> <span class="hljs-comment"># number of layers in the neural networks</span><br>    v = &#123;&#125;<br>    s = &#123;&#125;<br>    <br>    <span class="hljs-comment"># Initialize v, s. Input: &quot;parameters&quot;. Outputs: &quot;v, s&quot;.</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(L):<br>        v[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = np.zeros(parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)].shape)<br>        v[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = np.zeros(parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)].shape)<br>        s[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = np.zeros(parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)].shape)<br>        s[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = np.zeros(parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)].shape)<br>    <br>    <span class="hljs-keyword">return</span> v, s<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_parameters_with_adam</span>(<span class="hljs-params">parameters, grads, v, s, t, learning_rate = <span class="hljs-number">0.01</span>,</span><br><span class="hljs-params">                                beta1 = <span class="hljs-number">0.9</span>, beta2 = <span class="hljs-number">0.999</span>,  epsilon = <span class="hljs-number">1e-8</span></span>):<br>    <br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span>                 <span class="hljs-comment"># number of layers in the neural networks</span><br>    v_corrected = &#123;&#125;                         <span class="hljs-comment"># Initializing first moment estimate, python dictionary</span><br>    s_corrected = &#123;&#125;                         <span class="hljs-comment"># Initializing second moment estimate, python dictionary</span><br>    <br>    <span class="hljs-comment"># Perform Adam update on all parameters</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(L):<br>        <span class="hljs-comment"># Moving average of the gradients. Inputs: &quot;v, grads, beta1&quot;. Output: &quot;v&quot;.</span><br>        v[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = beta1*v[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] +(<span class="hljs-number">1</span>-beta1)*grads[<span class="hljs-string">&#x27;dW&#x27;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)]<br>        v[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = beta1*v[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] +(<span class="hljs-number">1</span>-beta1)*grads[<span class="hljs-string">&#x27;db&#x27;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)]<br><br>        <span class="hljs-comment"># Compute bias-corrected first moment estimate. Inputs: &quot;v, beta1, t&quot;. Output: &quot;v_corrected&quot;.</span><br>        v_corrected[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = v[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]/(<span class="hljs-number">1</span>-(beta1)**t)<br>        v_corrected[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = v[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]/(<span class="hljs-number">1</span>-(beta1)**t)<br><br>        <span class="hljs-comment"># Moving average of the squared gradients. Inputs: &quot;s, grads, beta2&quot;. Output: &quot;s&quot;.</span><br>        s[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] =beta2*s[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] + (<span class="hljs-number">1</span>-beta2)*(grads[<span class="hljs-string">&#x27;dW&#x27;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)]**<span class="hljs-number">2</span>)<br>        s[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = beta2*s[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] + (<span class="hljs-number">1</span>-beta2)*(grads[<span class="hljs-string">&#x27;db&#x27;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)]**<span class="hljs-number">2</span>)<br><br>        <span class="hljs-comment"># Compute bias-corrected second raw moment estimate. Inputs: &quot;s, beta2, t&quot;. Output: &quot;s_corrected&quot;.</span><br>        s_corrected[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] =s[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]/(<span class="hljs-number">1</span>-(beta2)**t)<br>        s_corrected[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = s[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]/(<span class="hljs-number">1</span>-(beta2)**t)<br><br>        <span class="hljs-comment"># Update parameters. Inputs: &quot;parameters, learning_rate, v_corrected, s_corrected, epsilon&quot;. Output: &quot;parameters&quot;.</span><br>        parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]-learning_rate*(v_corrected[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]/np.sqrt( s_corrected[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]+epsilon))<br>        parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]-learning_rate*(v_corrected[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]/np.sqrt( s_corrected[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]+epsilon))<br>        <br>    <span class="hljs-keyword">return</span> parameters, v, s<br></code></pre></td></tr></table></figure><table><thead><tr class="header"><th>优化方法</th><th>准确度</th><th>模型损失</th></tr></thead><tbody><tr class="odd"><td>Gradient descent</td><td>79.70％</td><td>振荡</td></tr><tr class="even"><td>Momentum</td><td>79.70％</td><td>振荡</td></tr><tr class="odd"><td>Adam</td><td>94％</td><td>更光滑</td></tr></tbody></table><p>冲量通常会有所帮助，但是鉴于学习率低和数据集过于简单，其影响几乎可以忽略不计。</p><p>同样，你看到损失的巨大波动是因为对于优化算法，某些小批处理比其他小批处理更为困难。</p><p>另一方面，Adam明显胜过小批次梯度下降和冲量。如果你在此简单数据集上运行更多epoch，则这三种方法都将产生非常好的结果。但是，Adam收敛得更快。</p><p>Adam的优势包括：</p><ol type="1"><li>相对较低的内存要求（尽管高于梯度下降和带冲量的梯度下降）</li><li>即使很少调整超参数，通常也能很好地工作（<spanclass="math inline">\(\alpha\)</span>除外）</li></ol><h3 id="超参数调整优化">2. 超参数调整优化</h3><h4 id="学习率衰减learning-rate-decay">2.1 学习率衰减(Learning RateDecay)</h4><p>如果使用固定的学习率 α, 在使用 mini-batch时在最后的迭代过程中会有噪音, 不会精确收敛, 最终一直在附近摆动.因此我们希望在训练后期 α不断减小.</p><p>以下为几个常见的方法:</p><p><strong>法一：</strong> <spanclass="math display">\[α=\frac{α_0}{decay\_rate∗epoch\_num}\]</span>其中 α0为初始学习率; epoch_num为当前迭代的代数; decay_rate是衰减率,一个需要调整的超参数.</p><p><strong>法二：</strong> <spanclass="math display">\[α=0.95^{epoch\_num}α_0\]</span> 其中 0.95自然也能是一些其他的小于 1 的数字.</p><p><strong>法三：</strong></p><p><spanclass="math display">\[α=\frac{k}{\sqrt{epoch\_num}}α_0\]</span></p><p><strong>法四:</strong></p><p>离散下降(discrete stair cease), 过一阵子学习率减半, 过一会又减半.<span class="math display">\[\alpha = \frac{1}{1 + decayRate \times\lfloor\frac{epochNum}{timeInterval}\rfloor} \alpha_{0}\]</span></p><p><strong>法五:</strong></p><p>手动衰减, 感觉慢了就调快点, 感觉快了就调慢点.</p><h3 id="局部最优问题local-optima">3. 局部最优问题(Local Optima)</h3><p>人们经常担心算法困在局部最优点, 而事实上算法更经常被困在鞍点,尤其是在高维空间中</p><p>成熟的优化算法如 Adam 算法，能够加快速度，让你尽早往下走出平稳段.</p>]]></content>
    
    
    <categories>
      
      <category>DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL</tag>
      
      <tag>optimize algorithms</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Some tricks in DL</title>
    <link href="/2023/01/26/Code/DL/Notes4/"/>
    <url>/2023/01/26/Code/DL/Notes4/</url>
    
    <content type="html"><![CDATA[<h2 id="basic-recipe-for-mldlc2.u1">Basic recipe for ML/DL(C2.U1)</h2><h4 id="concept">Concept</h4><ol type="1"><li><strong>data set</strong></li></ol><ul><li>training set 训练集</li><li>Dev(elop) set</li><li>Test set 测试集</li></ul><ol start="2" type="1"><li>欠拟合和过拟合 |欠拟合 | right|over过拟合 | |--- |--- |--- ||error大,i.e Bias| |方差varience大|</li></ol><p><img src="/img/imgDL/bias_varience.png" /> * <strong>From Bias toVarience</strong> &gt; Bias----&gt; training set &gt; Varience----&gt;Dev set</p><pre><code class=" mermaid">graph LRS1(light bias)--&gt;yes?--&gt;S2(light varience)S2 --&gt;S3(right)S2 --&gt;S4(high Bias &amp; high varience)S4 --&gt;S1</code></pre><p>改进过拟合:1.增加数据集(难！)(对于图片可以旋转裁剪扭曲等操作增加数据集,但可以减少数据获取，质量不如全新的图片，但方便)2.正则化</p><h2 id="regularzation-正则化">Regularzation 正则化</h2><p>target: <strong>lower the overfitting</strong></p><p>1.<strong>L1正则化</strong></p><p>使用L1正则化, w最终会是稀疏的(w中含很多0),有利于压缩模型但也没有降低太多内存,所以不能将压缩作为L1正则化的目的。通常我们使用<strong>L2正则化</strong>.<span class="math display">\[ J(w, b) = \frac{1}{m} \sum^{m}_{i=1}\mathcal{L}(a^{(i)}, y^{(i)}) + \frac{\lambda}{2m} ||w||_1 \]</span></p><p>其中<span class="math inline">\(||w||_1 = \sum^{n_x}_{j=1}|w_j|.\)</span></p><p>2 .<strong>L2正则化</strong> <span class="math display">\[ J(w,b) =\frac{1}{m}\sum_{i=1}^{m} yloga+(1-y)log(1-a)  + \frac{\lambda}{2m}||w||\]</span> 其中<span class="math inline">\(||w||^2_2 = \sum^{n_x}_{j=1}w_j^2 = w^Tw\)</span>,</p><p>注: 尽管<span class="math inline">\(b\)</span>也是参数,但我们没有必要添加 <span class="math inline">\(\fracλ{2m}b\)</span>项，因为<spanclass="math inline">\(w\)</span>几乎涵盖了所有参数, 而b只是众多参数中的一个, 可以忽略不计(当然加上也没问题).</p><p>为什么加上一个<spanclass="math inline">\(\frac{\lambda}{2m}||w||\)</span>会减少过拟合(overfitting)呢？</p><p>直观上来说，当 <spanclass="math inline">\(\lambda\)</span>较大时，迭代后||w||会变很小，因此某些神经元权重会变很小(甚至为0)，因此神经网络结构会变的简单(极端的话甚至趋于线性)。</p><p>3.<strong>dropout 随机失活(权值衰减)</strong></p><p>广泛用于computer vision缺点:难以保证J/L是否足够好,在调试与找参数时必要的关闭dropout由于L2正则化失活往往会把权重集中在一个或是少数神经元上,导致拟合效果不佳</p><p>4 其他正则化</p><p><strong>数据扩增:</strong></p><p>比如训练分类猫咪的图片,将图片左右翻转、旋转一个小角度、稍微变形处理等, 可以人工合成数据.</p><p><strong>Early Stopping:</strong></p><p>运行梯度下降时, 我们可以绘制训练误差, 当验证集误差不降反增的时候,停止训练.</p><p><strong>缺点</strong>：是可能导致代价J值不够小,却又没解决继续训练可能导致的过拟合问题.(且不利于训练的优化)</p><h3 id="code">Code</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_propagation_with_dropout</span>(<span class="hljs-params">X, parameters, keep_prob = <span class="hljs-number">0.8</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    X -- input dataset, of shape (input size, number of examples)</span><br><span class="hljs-string">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;,...,&quot;WL&quot;, &quot;bL&quot;</span><br><span class="hljs-string">                    W -- weight matrix of shape (size of current layer, size of previous layer)</span><br><span class="hljs-string">                    b -- bias vector of shape (size of current layer,1)</span><br><span class="hljs-string">    keep_prob: probability of keeping a neuron active during drop-out, scalar</span><br><span class="hljs-string">    :return:</span><br><span class="hljs-string">    AL: the output of the last Layer(y_predict)</span><br><span class="hljs-string">    caches: list, every element is a tuple:(W,b,z,A_pre)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    np.random.seed(<span class="hljs-number">1</span>)  <span class="hljs-comment">#random seed</span><br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span>            <span class="hljs-comment"># number of layer</span><br>    A = X<br>    caches = [(<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,<span class="hljs-literal">None</span>,X,<span class="hljs-literal">None</span>)]  <span class="hljs-comment"># 用于存储每一层的，w,b,z,A,D第0层w,b,z用none代替</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>        A_pre = A<br>        W = parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l)]<br>        b = parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l)]<br>        z = np.dot(W, A_pre) + b  <span class="hljs-comment"># 计算z = wx + b</span><br>        A = relu(z)  <span class="hljs-comment"># relu activation function</span><br>        D = np.random.rand(A.shape[<span class="hljs-number">0</span>], A.shape[<span class="hljs-number">1</span>]) <span class="hljs-comment">#initialize matrix D</span><br>        D = (D &lt; keep_prob)       <span class="hljs-comment">#convert entries of D to 0 or 1 (using keep_prob as the threshold)</span><br>        A = np.multiply(A, D)     <span class="hljs-comment">#shut down some neurons of A</span><br>        A = A / keep_prob         <span class="hljs-comment"># scale the value of neurons that haven&#x27;t been shut down</span><br>        caches.append((W, b, z, A,D))<br>    <span class="hljs-comment"># calculate Lth layer</span><br>    WL = parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(L)]<br>    bL = parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(L)]<br>    zL = np.dot(WL, A) + bL<br>    AL = sigmoid(zL)<br>    caches.append((WL, bL, zL, A))<br>    <span class="hljs-keyword">return</span> AL, caches<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">backward_propagation_with_dropout</span>(<span class="hljs-params">AL, Y, caches, keep_prob = <span class="hljs-number">0.8</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Implement the backward propagation presented in figure 2.</span><br><span class="hljs-string">        Arguments:</span><br><span class="hljs-string">        X -- input dataset, of shape (input size, number of examples)</span><br><span class="hljs-string">        Y -- true &quot;label&quot; vector (containing 0 if cat, 1 if non-cat)</span><br><span class="hljs-string">        caches -- caches output from forward_propagation(),(W,b,z,pre_A)</span><br><span class="hljs-string">        keep_prob: probability of keeping a neuron active during drop-out, scalar</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">        gradients -- A dictionary with the gradients with respect to dW,db</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>    m = Y.shape[<span class="hljs-number">1</span>]<br>    L = <span class="hljs-built_in">len</span>(caches) - <span class="hljs-number">1</span><br>    <span class="hljs-comment"># print(&quot;L:   &quot; + str(L))</span><br>    <span class="hljs-comment"># calculate the Lth layer gradients</span><br>    prev_AL = caches[L - <span class="hljs-number">1</span>][<span class="hljs-number">3</span>]<br>    dzL = <span class="hljs-number">1.</span> / m * (AL - Y)<br>    dWL = np.dot(dzL, prev_AL.T)<br>    dbL = np.<span class="hljs-built_in">sum</span>(dzL, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br>    gradients = &#123;<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(L): dWL, <span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(L): dbL&#125;<br>    <span class="hljs-comment"># calculate from L-1 to 1 layer gradients</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L)): <span class="hljs-comment"># L-1,L-2,...,1</span><br>        post_W = caches[l + <span class="hljs-number">1</span>][<span class="hljs-number">0</span>]  <span class="hljs-comment"># 要用后一层的W</span><br>        dz = dzL  <span class="hljs-comment"># 用后一层的dz</span><br>        dal = np.dot(post_W.T, dz)<br>        Dl = caches[l][<span class="hljs-number">4</span>] <span class="hljs-comment">#当前层的D</span><br>        dal = np.multiply(dal, Dl) <span class="hljs-comment">#Apply mask Dl to shut down the same neurons as during the forward propagation</span><br>        dal = dal / keep_prob <span class="hljs-comment">#Scale the value of neurons that haven&#x27;t been shut down</span><br>        Al = caches[l][<span class="hljs-number">3</span>]  <span class="hljs-comment">#当前层的A</span><br>        dzl = np.multiply(dal, relu_backward(Al))<span class="hljs-comment">#也可以用dzl=np.multiply(dal, np.int64(Al &gt; 0))来实现</span><br>        prev_A = caches[l-<span class="hljs-number">1</span>][<span class="hljs-number">3</span>]  <span class="hljs-comment"># 前一层的A</span><br>        dWl = np.dot(dzl, prev_A.T)<br>        dbl = np.<span class="hljs-built_in">sum</span>(dzl, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>)<br><br>        gradients[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l)] = dWl<br>        gradients[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l)] = dbl<br>        dzL = dzl  <span class="hljs-comment"># 更新dz</span><br>    <span class="hljs-keyword">return</span> gradients    <br></code></pre></td></tr></table></figure><blockquote><p>注意： 1.训练时的 "a[l]/= 0.8" 要修复权重</p><p>2.在测试阶段无需使用 Dropout.(测试阶段要关掉)</p><p>3.Dropout 不能与梯度检验同时使用，因为 Dropout在梯度下降上的代价函数J难以计算. * 结果展示 Here are the results of ourthree models:</p></blockquote><table><thead><tr class="header"><th>model</th><th>train accuracy</th><th>test accuracy</th></tr></thead><tbody><tr class="odd"><td>3-layer NN without regularization</td><td>95%</td><td>91.5%</td></tr><tr class="even"><td>3-layer NN with L2-regularization</td><td>94%</td><td>93%</td></tr><tr class="odd"><td>3-layer NN with dropout</td><td>93%</td><td>95%</td></tr></tbody></table><h2 id="优化设置">优化设置</h2><ul><li><strong>归一化输入</strong> <span class="math display">\[\overrightarrow x = x-\mu  , \overrightarrow x/=\sigma\]</span> <spanclass="math display">\[其中 \mu= \frac1m\sum x ,\sigma ^2=\frac1mx^2    \]</span> 优化效果如下:即使各维度尺寸保持相对一致便于调控搜索的步长 <imgsrc="/img/imgDL/normalization.png" /></li><li><strong>权值初始化</strong> 处理梯度消失&amp;梯度爆炸<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs py">W[l] = np.random.randn(shape)*np.sqrt(<span class="hljs-number">2</span>/n[l-<span class="hljs-number">1</span>])<br>\<span class="hljs-comment"># 使用Relu作为激活函数,不同激活函数可查询资料/</span><br></code></pre></td></tr></table></figure></li></ul><h2 id="梯度检测">梯度检测</h2><blockquote><p>---only to debug, don't in training</p><p>梯度估计:<spanclass="math inline">\(f^{&#39;}(x)=\frac{f(x+\epsilon)-f(x-\epsilon)}{2\epsilon}\)</span>,数值计算比单侧导数更精确.</p></blockquote><p>1.1 gradient checking 如何工作?</p><p>Backpropagation 中计算梯度(the gradients) <spanclass="math inline">\(∂J,∂θ,θ\)</span>代表着模型的参数，<spanclass="math inline">\(J\)</span>是使用前向传播和你的lossfunction来计算的。前向传播十分容易，因此使用计算<spanclass="math inline">\(J\)</span>的代码来确认计算 <spanclass="math inline">\(\frac{∂J}{∂θ}\)</span>的代码</p><p>我们来看一下derivative (or gradient)的定义：</p><p><spanclass="math display">\[\frac{∂J}{∂θ}=lim_{ε→0}\frac{J(θ+ε)−J(θ−ε)}{2ε}\]</span>接下来： 计算 "gradapprox" 使用公式（1）和 一个很小的值 ε.遵循以下步骤:</p><ol type="1"><li><p><span class="math inline">\(θ^+=θ+ε\)</span></p></li><li><p><span class="math inline">\(θ^−=θ−ε\)</span></p></li><li><p><span class="math inline">\(J^+=J(θ+)\)</span></p></li><li><p><span class="math inline">\(J^−=J(θ−)\)</span></p></li><li><p><spanclass="math inline">\(gradapprox=\frac{J^+−J^−}{2ε}\)</span></p></li></ol><p>然后，使用backward propagation计算gradient , 并存储结果到变量 "grad"最后, 计算 "gradapprox" 和 the "grad" 的相对偏差，使用下列公式: <spanclass="math display">\[difference=\frac{||grad−gradapprox||_2}{||grad||_2+||gradapprox||_2}\]</span>需要三个步骤来计算这个公式： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-comment">#1&#x27;. compute the numerator(分子) </span><br> np.linalg.norm(...)<br><span class="hljs-comment">#2&#x27;. compute the denominator(分母). </span><br> np.linalg.norm(...) <br><span class="hljs-comment">#3&#x27;. divide them.</span><br></code></pre></td></tr></table></figure> 如果这个 difference 非常小(小于 <span class="math inline">\(10^{−7}\)</span>), gradient计算正确.否则，错误. <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">gradient_check</span>(<span class="hljs-params">x, theta, epsilon = <span class="hljs-number">1e-7</span></span>):<br>   <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">   Implement the backward propagation presented in Figure 1.</span><br><span class="hljs-string">   &quot;&quot;&quot;</span><br>   <br>   <span class="hljs-comment"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don&#x27;t need to worry about the limit.</span><br>   thetaplus = theta + epsilon<br>   thetaminus = theta - epsilon<br>   J_plus = forward_propagation(x, thetaplus)<br>   J_minus = forward_propagation(x, thetaminus)<br>   gradapprox = (J_plus - J_minus) / (<span class="hljs-number">2.</span> * epsilon)<br><br>   <span class="hljs-comment"># Check if gradapprox is close enough to the output of backward_propagation()</span><br>   grad = backward_propagation(x, theta)<br>   numerator = np.linalg.norm(grad - gradapprox)<br>   denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)<br>   difference = numerator / denominator<br>   <br>   <span class="hljs-keyword">if</span> difference &lt; <span class="hljs-number">1e-7</span>:<br>       <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;The gradient is correct!&quot;</span>)<br>   <span class="hljs-keyword">else</span>:<br>       <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;The gradient is wrong!&quot;</span>)<br>   <span class="hljs-keyword">return</span> difference<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL</tag>
      
      <tag>正则化</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DL基本原理</title>
    <link href="/2023/01/24/Code/DL/DL%E5%8E%9F%E7%90%86/"/>
    <url>/2023/01/24/Code/DL/DL%E5%8E%9F%E7%90%86/</url>
    
    <content type="html"><![CDATA[<p>本文将基于吴恩达(Andrew)的网课第一部分内容对深度学习(deeplearning)基础原理进行阐述.</p><h1 id="network-therom">Network Therom</h1><h2 id="向前传播forward-propogation">向前传播forward propogation</h2><p>这次笔记主要是对向前传播的过程以及数学公式进行梳理和整合</p><h3 id="单个神经元">1. 单个神经元</h3><p><img src="/img/imgDL/neuron_cell.png" alt="cell" /> x,w 是列向量<spanclass="math inline">\((x_{1\times 3})\)</span>,<spanclass="math inline">\(\sigma\)</span>是激活函数，作用是使之非线性化，具体在另一篇notes[]中提到</p><h3 id="单层网络">2. 单层网络</h3><p><img src="/img/imgDL/layer1.png" />为避免循环出现我们采用向量化方法,i.e <span class="math display">\[z =W^T x+b, a=g(z),\\其中W=[w_1,w2,...w_{n_h}]\]</span>进一步的，由于我们有m组数据,故将z,x扩充成matrix,i.e <spanclass="math display">\[Z=[z_1,z_2...z_m],X=[x_1,x_2,...x_m]\]</span></p><h3 id="多层网络">3. 多层网络</h3><p><span class="math inline">\(A(Z)^{[i]}_{nh_i\timesm}\)</span>表示第i层的数据，<spanclass="math inline">\(W^{T[i]}_{nh_i\timesnh_{i-1}}\)</span>是第i层的传输矩阵注意矩阵规格，则有 <spanclass="math display">\[Z^{[i]}=W^{T[i]}A^{[i-1]}+b^{[i]},A^i=g(Z^i)\]</span>传到最后一层A会退化成一个1xm的行向量，我们在计算J和L.</p><h3 id="code">Code:</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_forward</span>(<span class="hljs-params">A, W, b</span>):<br>    Z = np.dot(W,A) + b<br>    <span class="hljs-keyword">assert</span>(Z.shape == (W.shape[<span class="hljs-number">0</span>], A.shape[<span class="hljs-number">1</span>]))<br>    cache = (A, W, b)<br>    <span class="hljs-keyword">return</span> Z, cache<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_activation_forward</span>(<span class="hljs-params">A_prev, W, b, activation</span>):<br>    <span class="hljs-keyword">if</span> activation == <span class="hljs-string">&quot;sigmoid&quot;</span>:<br>        <span class="hljs-comment"># Inputs: &quot;A_prev, W, b&quot;. Outputs: &quot;A, activation_cache&quot;.</span><br>        Z, linear_cache = linear_forward(A_prev,W,b)<br>        A, activation_cache = sigmoid(Z)<br>    <span class="hljs-keyword">elif</span> activation == <span class="hljs-string">&quot;relu&quot;</span>:<br>        Z, linear_cache = linear_forward(A_prev,W,b)<br>        A, activation_cache = relu(Z)<br>    <span class="hljs-keyword">assert</span> (A.shape == (W.shape[<span class="hljs-number">0</span>], A_prev.shape[<span class="hljs-number">1</span>]))<br>    cache = (linear_cache, activation_cache)<br><br>    <span class="hljs-keyword">return</span> A, cache<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">L_model_forward</span>(<span class="hljs-params">X, parameters</span>):<br>    caches = []<br>    A = X<br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span>                  <span class="hljs-comment"># number of layers in the neural network</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, L):<br>        A_prev = A <br>        A, cache = linear_activation_forward(A_prev,parameters[<span class="hljs-string">&#x27;W&#x27;</span> + <span class="hljs-built_in">str</span>(l)],parameters[<span class="hljs-string">&#x27;b&#x27;</span> + <span class="hljs-built_in">str</span>(l)],activation = <span class="hljs-string">&quot;relu&quot;</span>)<br>        caches.append(cache)<br>    AL, cache = linear_activation_forward(A,parameters[<span class="hljs-string">&#x27;W&#x27;</span> + <span class="hljs-built_in">str</span>(L)],parameters[<span class="hljs-string">&#x27;b&#x27;</span> + <span class="hljs-built_in">str</span>(L)],activation = <span class="hljs-string">&quot;sigmoid&quot;</span>)<br>    caches.append(cache)<br>    <span class="hljs-keyword">assert</span>(AL.shape == (<span class="hljs-number">1</span>,X.shape[<span class="hljs-number">1</span>]))<br>    <span class="hljs-keyword">return</span> AL, caches<br></code></pre></td></tr></table></figure><h2 id="loss计算">Loss计算</h2><p><span class="math display">\[J=\frac{1}{m}\sum -ylog\haty-(1-y)log(1-\hat y)\]</span> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_cost</span>(<span class="hljs-params">AL, Y</span>):<br>    m = Y.shape[<span class="hljs-number">1</span>]<br>    <span class="hljs-comment"># Compute loss from aL and y.</span><br>    cost = -<span class="hljs-number">1</span> / m * np.<span class="hljs-built_in">sum</span>(Y * np.log(AL) + (<span class="hljs-number">1</span>-Y) * np.log(<span class="hljs-number">1</span>-AL),axis=<span class="hljs-number">1</span>,keepdims=<span class="hljs-literal">True</span>)<br>    cost = np.squeeze(cost)      <span class="hljs-comment"># To make sure your cost&#x27;s shape is what we expect (e.g. this turns [[17]] into 17).</span><br>    <span class="hljs-keyword">assert</span>(cost.shape == ())<br>    <span class="hljs-keyword">return</span> cost<br></code></pre></td></tr></table></figure></p><h2 id="反向传播">反向传播</h2><p><span class="math inline">\(dx记为\frac{\partial J}{\partialx},取损失函数L=-ylog\hat y-(1-y)log(1-\hat y),J=\frac{1}{m}\sum L(y,\haty) ,\)</span> $a=g(z)为sigmoid函数时,g<sup>{'}(z)==g(z)(1-g(z))=a-a</sup>2,Relu函数,g^{'}(z)=1.$<span class="math inline">\(da^{[l]}=\frac1m\sum-\frac{y}{a}+\frac{1-y}{1-a}\)</span> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs py">da_l=<span class="hljs-number">1</span>/m*np.<span class="hljs-built_in">sum</span>(-y/a+(<span class="hljs-number">1</span>-y)/(<span class="hljs-number">1</span>-a))<br></code></pre></td></tr></table></figure> <spanclass="math inline">\(dz^{[l]}=da^{[l]}*\frac{\partial a}{\partialz}=a-a^2\)</span> 再向前传播 <spanclass="math inline">\(da^{[n]}=dz^{[n+1]}\timesW^{T[n+1]},dz^{[n]}=da^{[n]}\times\frac{\partial a}{\partialz},dw^{[n]}=dz^{[n+1]}\times a^{[n-1]},db^{[n]}=dz^{[n]}\)</span>其中为了计算我们可以吧W,A存在cache中 <imgsrc="/img/imgDL/nueron_net.png" /></p><h3 id="code-1">Code:</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_backward</span>(<span class="hljs-params">dZ, cache</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Implement the linear portion of backward propagation for a single layer (layer l)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    A_prev, W, b = cache<br>    m = A_prev.shape[<span class="hljs-number">1</span>]<br>    dW = <span class="hljs-number">1</span> / m * np.dot(dZ ,A_prev.T)<br>    db = <span class="hljs-number">1</span> / m * np.<span class="hljs-built_in">sum</span>(dZ,axis = <span class="hljs-number">1</span> ,keepdims=<span class="hljs-literal">True</span>)<br>    dA_prev = np.dot(W.T,dZ) <br>    <br>    <span class="hljs-keyword">assert</span> (dA_prev.shape == A_prev.shape)<br>    <span class="hljs-keyword">assert</span> (dW.shape == W.shape)<br>    <span class="hljs-keyword">assert</span> (db.shape == b.shape)<br>    <br>    <span class="hljs-keyword">return</span> dA_prev, dW, db<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">L_model_backward</span>(<span class="hljs-params">AL, Y, caches</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    grads = &#123;&#125;<br>    L = <span class="hljs-built_in">len</span>(caches) <span class="hljs-comment"># the number of layers</span><br>    m = AL.shape[<span class="hljs-number">1</span>]<br>    Y = Y.reshape(AL.shape) <span class="hljs-comment"># after this line, Y is the same shape as AL</span><br><br>    <span class="hljs-comment"># Initializing the backpropagation</span><br>    dAL = - (np.divide(Y, AL) - np.divide(<span class="hljs-number">1</span> - Y, <span class="hljs-number">1</span> - AL))<br>    <br>    <span class="hljs-comment"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;AL, Y, caches&quot;. Outputs: &quot;grads[&quot;dAL&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span><br><br>    current_cache = caches[L-<span class="hljs-number">1</span>]<br>    grads[<span class="hljs-string">&quot;dA&quot;</span> + <span class="hljs-built_in">str</span>(L)], grads[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(L)], grads[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(L)] = linear_activation_backward(dAL, current_cache, activation = <span class="hljs-string">&quot;sigmoid&quot;</span>)<br>      <br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-built_in">range</span>(L - <span class="hljs-number">1</span>)):<br>        <span class="hljs-comment"># lth layer: (RELU -&gt; LINEAR) gradients.</span><br>        <span class="hljs-comment"># Inputs: &quot;grads[&quot;dA&quot; + str(l + 2)], caches&quot;. Outputs: &quot;grads[&quot;dA&quot; + str(l + 1)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span><br>        current_cache = caches[l]<br>        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="hljs-string">&quot;dA&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">2</span>)], current_cache, activation = <span class="hljs-string">&quot;relu&quot;</span>)<br>        grads[<span class="hljs-string">&quot;dA&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = dA_prev_temp<br>        grads[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = dW_temp<br>        grads[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)] = db_temp<br>    <span class="hljs-keyword">return</span> grads<br></code></pre></td></tr></table></figure><h2 id="梯度下降">梯度下降</h2><p>upgrade:主要是对参数W,b进行修正,其中$$为学习率,后面我们会对这部分进行优化算法改进,此处不赘述. <span class="math display">\[W=W-\alpha dW,b=b-\alphadb\]</span> <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_parameters</span>(<span class="hljs-params">parameters, grads, learning_rate</span>):<br>    L = <span class="hljs-built_in">len</span>(parameters) // <span class="hljs-number">2</span> <span class="hljs-comment"># number of layers in the neural network</span><br>    <span class="hljs-comment"># Update rule for each parameter. Use a for loop.</span><br>    <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(L):<br>        parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)] =  parameters[<span class="hljs-string">&quot;W&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)] - learning_rate * grads[<span class="hljs-string">&quot;dW&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]<br>        parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)] = parameters[<span class="hljs-string">&quot;b&quot;</span> + <span class="hljs-built_in">str</span>(l+<span class="hljs-number">1</span>)] - learning_rate * grads[<span class="hljs-string">&quot;db&quot;</span> + <span class="hljs-built_in">str</span>(l + <span class="hljs-number">1</span>)]<br>        <br>    <span class="hljs-keyword">return</span> parameters<br></code></pre></td></tr></table></figure></p>]]></content>
    
    
    <categories>
      
      <category>DL</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VSCode远程SSH连接</title>
    <link href="/2023/01/24/Code/env/vscode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <url>/2023/01/24/Code/env/vscode%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
    
    <content type="html"><![CDATA[<p>环境:</p><ul><li>本地: windows11</li><li>腾讯云服务器: ubuntu 20.04</li></ul><ol type="1"><li><p>首先在腾讯云平台创建密钥并将公钥与你要用的实例绑定,下载私钥到本机(地址假设为D://tcf//tcf.pem),</p><blockquote><p>注意:密钥绑定的是名为ubuntu的用户,所以你可以先在实例上切换用户为ubuntu观察<code>sudo su ubuntu</code></p></blockquote></li><li><p>在VSCode上下载插件Renote-SSH,然后配置ssh-configration</p></li></ol><figure><img src="/img/blogssh1.png" alt="blog-ssh1" /><figcaption aria-hidden="true">blog-ssh1</figcaption></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">Host tcf           //你的名字，可自取<br>    HostName 你的IP      <br>    Port 22             //SSH连接端口为22<br>    User ubuntu       //默认<br>    IdentityFile D:\tcf\tcf.pem   //你的私钥地址<br></code></pre></td></tr></table></figure><ol type="1"><li>新建一个终端，运行<code>ssh tcf</code></li></ol><p>然后Warning： 说是权限tooopen无法进入，大概是windows给用户的权限太多导致的一些问题</p><figure><img src="/img/blog-ssh2.png" alt="blog-ssh2" /><figcaption aria-hidden="true">blog-ssh2</figcaption></figure><p><ahref="%5B(22条消息)%20windows%20ssh%20Permissions%20for%20“xxx“%20are%20too%20open错误详细解决方案_菜到不知所措的博客-CSDN博客_are%20too%20open%5D(https://blog.csdn.net/weixin_40415591/article/details/121661857)">参考博客</a></p><p>最后保证你的权限里只有所有者一个即可。</p><figure><img src="/img/blog-ssh3.png" alt="blog-ssh3" /><figcaption aria-hidden="true">blog-ssh3</figcaption></figure><p>运行<code>ssh tcf</code> ,应该能正常登陆了</p><ol start="4" type="1"><li><p>再点击左下角远程连接</p><p>然后会帮你在虚拟机端安装VSCode-Server(大概几分钟),然后就可以用VSCode进行远程开发了。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>环境</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VSCode</tag>
      
      <tag>远程SSH</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>lambda 表达式 in C++</title>
    <link href="/2023/01/22/Code/CPP/lamba%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <url>/2023/01/22/Code/CPP/lamba%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
    
    <content type="html"><![CDATA[<p><strong>version</strong>: C++11引入</p><p><strong>优点：</strong></p><ul><li>声明式编程风格：就地匿名定义目标函数或函数对象，不需要额外写一个命名函数或者函数对象。(函数式编程的特点)</li><li>简洁且方便与上下文联系，有好的可读性和可维护性。</li><li>在需要的时间和地点实现功能闭包，使程序更灵活</li></ul><h3 id="lambda-表达式的概念和基本用法">lambda表达式的概念和基本用法</h3><p>lambda 表达式的语法形式可简单归纳如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">[ capture ] ( params ) opt -&gt; ret &#123; body; &#125;;<br></code></pre></td></tr></table></figure><p>其中 capture 是捕获列表，params 是参数表( C++14后支持auto类型 )，opt是函数选项，ret 是返回值类型，body是函数体</p><p>eg.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">auto</span> f = [](<span class="hljs-type">int</span> a) -&gt; <span class="hljs-type">int</span> &#123; <span class="hljs-keyword">return</span> a + <span class="hljs-number">1</span>; &#125;;<br></code></pre></td></tr></table></figure><h3 id="使用-lambda-表达式捕获列表">使用 lambda 表达式捕获列表</h3><p>lambda表达式还可以通过捕获列表捕获一定范围内的变量，因此可以使用或是修改这些变量：</p><ul><li>[] 不捕获任何变量。</li><li>[&amp;]捕获外部作用域中所有变量，并作为引用在函数体中使用（按引用捕获）。</li><li>[=]捕获外部作用域中所有变量，并作为副本在函数体中使用（按值捕获,不能修改此值否则会报错）。</li></ul><blockquote><p>如果实在需要改变lambda中的值，这时就需要使用上文提到过的选项mutable。默认情况下，lambda函数是一个const函数，而mutable也可以取消常量性</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-type">int</span> a = <span class="hljs-number">5</span>;<br><span class="hljs-keyword">auto</span> f = [=]()<span class="hljs-keyword">mutable</span>&#123;<span class="hljs-keyword">return</span> a*=<span class="hljs-number">5</span>;&#125;;<span class="hljs-comment">//取消常量性,不加mutable会报错</span><br>cout &lt;&lt; <span class="hljs-built_in">f</span>() &lt;&lt; endl;<br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></blockquote><ul><li>[=，&amp;foo] 按值捕获外部作用域中所有变量，并按引用捕获 foo变量。</li><li>[bar] 按值捕获 bar 变量，同时不捕获其他变量。</li><li>[this] 捕获当前类中的 this 指针，让 lambda表达式拥有和当前类成员函数同样的访问权限。如果已经使用了 &amp; 或者=，就默认添加此选项。捕获 this 的目的是可以在 lamda中使用当前类的成员函数和成员变量。</li></ul><h3 id="效率">效率</h3><p>使用lambda函数的效率与使用函数对象是一样的，都要快于函数指针。他们都能够在编译期将代码内联展开，减少函数调用的时间。</p><h3 id="实例">实例</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">A</span><br>&#123;<br>    <span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">int</span> i_ = <span class="hljs-number">0</span>;<br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">func</span><span class="hljs-params">(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y)</span></span><br><span class="hljs-function">    </span>&#123;<br>        <span class="hljs-keyword">auto</span> x1 = []&#123; <span class="hljs-keyword">return</span> i_; &#125;;                    <span class="hljs-comment">// error，没有捕获外部变量</span><br>        <span class="hljs-keyword">auto</span> x2 = [=]&#123; <span class="hljs-keyword">return</span> i_ + x + y; &#125;;           <span class="hljs-comment">// OK，捕获所有外部变量</span><br>        <span class="hljs-keyword">auto</span> x3 = [&amp;]&#123; <span class="hljs-keyword">return</span> i_ + x + y; &#125;;           <span class="hljs-comment">// OK，捕获所有外部变量</span><br>        <span class="hljs-keyword">auto</span> x4 = [<span class="hljs-keyword">this</span>]&#123; <span class="hljs-keyword">return</span> i_; &#125;;                <span class="hljs-comment">// OK，捕获this指针</span><br>        <span class="hljs-keyword">auto</span> x5 = [<span class="hljs-keyword">this</span>]&#123; <span class="hljs-keyword">return</span> i_ + x + y; &#125;;        <span class="hljs-comment">// error，没有捕获x、y</span><br>        <span class="hljs-keyword">auto</span> x6 = [<span class="hljs-keyword">this</span>, x, y]&#123; <span class="hljs-keyword">return</span> i_ + x + y; &#125;;  <span class="hljs-comment">// OK，捕获this指针、x、y</span><br>        <span class="hljs-keyword">auto</span> x7 = [<span class="hljs-keyword">this</span>]&#123; <span class="hljs-keyword">return</span> i_++; &#125;;              <span class="hljs-comment">// OK，捕获this指针，并修改成员的值</span><br>    &#125;<br>&#125;;<br><span class="hljs-type">int</span> a = <span class="hljs-number">0</span>, b = <span class="hljs-number">1</span>;<br><span class="hljs-keyword">auto</span> f1 = []&#123; <span class="hljs-keyword">return</span> a; &#125;;               <span class="hljs-comment">// error，没有捕获外部变量</span><br><span class="hljs-keyword">auto</span> f2 = [&amp;]&#123; <span class="hljs-keyword">return</span> a++; &#125;;            <span class="hljs-comment">// OK，捕获所有外部变量，并对a执行自加运算</span><br><span class="hljs-keyword">auto</span> f3 = [=]&#123; <span class="hljs-keyword">return</span> a; &#125;;              <span class="hljs-comment">// OK，捕获所有外部变量，并返回a</span><br><span class="hljs-keyword">auto</span> f4 = [=]&#123; <span class="hljs-keyword">return</span> a++; &#125;;            <span class="hljs-comment">// error，a是以复制方式捕获的，无法修改</span><br><span class="hljs-keyword">auto</span> f5 = [a]&#123; <span class="hljs-keyword">return</span> a + b; &#125;;          <span class="hljs-comment">// error，没有捕获变量b</span><br><span class="hljs-keyword">auto</span> f6 = [a, &amp;b]&#123; <span class="hljs-keyword">return</span> a + (b++); &#125;;  <span class="hljs-comment">// OK，捕获a和b的引用，并对b做自加运算</span><br><span class="hljs-keyword">auto</span> f7 = [=, &amp;b]&#123; <span class="hljs-keyword">return</span> a + (b++); &#125;;  <span class="hljs-comment">// OK，捕获所有外部变量和b的引用，并对b做自加运算</span><br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">int</span> a = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">auto</span> f = [=]&#123; <span class="hljs-keyword">return</span> a; &#125;;      <span class="hljs-comment">// 按值捕获外部变量</span><br><span class="hljs-keyword">auto</span> g = [&amp;]&#123; <span class="hljs-keyword">return</span> a; &#125;;      <span class="hljs-comment">// 按值捕获外部变量</span><br>a += <span class="hljs-number">1</span>;                         <span class="hljs-comment">// a被修改了</span><br>std::cout &lt;&lt; <span class="hljs-built_in">f</span>() &lt;&lt;<span class="hljs-built_in">g</span>() &lt;&lt; std::endl;  <span class="hljs-comment">// 输出01,注意.</span><br></code></pre></td></tr></table></figure><h3 id="lambda-表达式的类型">lambda 表达式的类型</h3><p>lambda 表达式的类型在 C++11 中被称为“闭包类型（ClosureType）”。它是一个特殊的，匿名的非 nunion 的类类型。</p><p>因此，我们可以认为它是一个带有 operator()的类，即仿函数。因此，我们可以使用 std::function 和 std::bind来存储和操作 lambda 表达式：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">std::function&lt;<span class="hljs-type">int</span>(<span class="hljs-type">int</span>)&gt;  f1 = [](<span class="hljs-type">int</span> a)&#123; <span class="hljs-keyword">return</span> a; &#125;;<br>std::function&lt;<span class="hljs-type">int</span>(<span class="hljs-type">void</span>)&gt; f2 = std::<span class="hljs-built_in">bind</span>([](<span class="hljs-type">int</span> a)&#123; <span class="hljs-keyword">return</span> a; &#125;, <span class="hljs-number">123</span>);<br></code></pre></td></tr></table></figure><p>另外，对于没有捕获任何变量的 lambda表达式，还可以被转换成一个普通的函数指针：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">using</span> <span class="hljs-type">func_t</span> = <span class="hljs-built_in">int</span>(*)(<span class="hljs-type">int</span>);<br><span class="hljs-type">func_t</span> f = [](<span class="hljs-type">int</span> a)&#123; <span class="hljs-keyword">return</span> a; &#125;;<br><span class="hljs-built_in">f</span>(<span class="hljs-number">123</span>);<br></code></pre></td></tr></table></figure><p>lambda表达式可以说是就地定义仿函数闭包的“语法糖”。它的捕获列表捕获住的任何外部变量，最终均会变为闭包类型的成员变量。而一个使用了成员变量的类的operator()，如果能直接被转换为普通的函数指针，那么 lambda 表达式本身的this 指针就丢失掉了。而没有捕获任何外部变量的 lambda表达式则不存在这个问题。</p><h4 id="转化为函数指针">转化为函数指针</h4><p>需要注意的是，没有捕获变量的 lambda表达式可以直接转换为函数指针，而捕获变量的 lambda表达式则不能转换为函数指针。看看下面的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">typedef</span> <span class="hljs-title">void</span><span class="hljs-params">(*Ptr)</span><span class="hljs-params">(<span class="hljs-type">int</span>*)</span></span>;<br>Ptr p = [](<span class="hljs-type">int</span>* p)&#123;<span class="hljs-keyword">delete</span> p;&#125;;  <span class="hljs-comment">// 正确，没有状态的lambda（没有捕获）的lambda表达式可以直接转换为函数指针</span><br>Ptr p1 = [&amp;](<span class="hljs-type">int</span>* p)&#123;<span class="hljs-keyword">delete</span> p;&#125;;  <span class="hljs-comment">// 错误，有状态的lambda不能直接转换为函数指针</span><br></code></pre></td></tr></table></figure><h3 id="实例lambda-表达式代替函数对象的示例">【实例】lambda表达式代替函数对象的示例。</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CountEven</span><br>&#123;<br>    <span class="hljs-type">int</span>&amp; count_;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">CountEven</span>(<span class="hljs-type">int</span>&amp; count) : <span class="hljs-built_in">count_</span>(count) &#123;&#125;<br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span><span class="hljs-params">(<span class="hljs-type">int</span> val)</span></span><br><span class="hljs-function">    </span>&#123;<br>        <span class="hljs-keyword">if</span> (!(val &amp; <span class="hljs-number">1</span>))       <span class="hljs-comment">// val % 2 == 0</span><br>        &#123;<br>            ++ count_;<br>        &#125;<br>    &#125;<br>&#125;;<br>std::vector&lt;<span class="hljs-type">int</span>&gt; v = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span> &#125;;<br><span class="hljs-type">int</span> even_count = <span class="hljs-number">0</span>;<br>for_each(v.<span class="hljs-built_in">begin</span>(), v.<span class="hljs-built_in">end</span>(), <span class="hljs-built_in">CountEven</span>(even_count));<br>std::cout &lt;&lt; <span class="hljs-string">&quot;The number of even is &quot;</span> &lt;&lt; even_count &lt;&lt; std::endl;<br></code></pre></td></tr></table></figure><p>用lambda函数:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp">std::vector&lt;<span class="hljs-type">int</span>&gt; v = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span> &#125;;<br><span class="hljs-type">int</span> even_count = <span class="hljs-number">0</span>;<br>for_each( v.<span class="hljs-built_in">begin</span>(), v.<span class="hljs-built_in">end</span>(), [&amp;even_count](<span class="hljs-type">int</span> val)<br>        &#123;<br>            <span class="hljs-keyword">if</span> (!(val &amp; <span class="hljs-number">1</span>))  <span class="hljs-comment">// val % 2 == 0</span><br>            &#123;<br>                ++ even_count;<br>            &#125;<br>        &#125;);<br>std::cout &lt;&lt; <span class="hljs-string">&quot;The number of even is &quot;</span> &lt;&lt; even_count &lt;&lt; std::endl;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>CPP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>lambda表达式</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>C/C++基础补充</title>
    <link href="/2023/01/22/Code/CPP/cpp_study/"/>
    <url>/2023/01/22/Code/CPP/cpp_study/</url>
    
    <content type="html"><![CDATA[<h1 id="cc-知识巩固">C/C++ 知识巩固</h1><h3 id="c函数指针">C函数指针</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-comment">//定义一个函数指针pFUN，它指向一个返回类型为char，有一个整型的参数的函数</span><br><span class="hljs-built_in">char</span> (*pFun)(<span class="hljs-type">int</span>);<br><span class="hljs-comment">//定义一个返回类型为char，参数为int的函数</span><br><span class="hljs-comment">//从指针层面上理解该函数，即函数的函数名实际上是一个指针，</span><br><span class="hljs-comment">//该指针指向函数在内存中的首地址</span><br><span class="hljs-function"><span class="hljs-type">char</span> <span class="hljs-title">glFun</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span><br><span class="hljs-function"></span>&#123;<br>    cout &lt;&lt; a;<br>    <span class="hljs-comment">//return a;</span><br>&#125;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br><span class="hljs-comment">//将函数glFun的地址赋值给变量pFun</span><br>    pFun = glFun;<br><span class="hljs-comment">//*pFun”显然是取pFun所指向地址的内容，当然也就是取出了函数glFun()的内容，然后给定参数为2。</span><br>    (*pFun)(<span class="hljs-number">2</span>);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><p>用而typedef可以让函数指针更直观方便</p><p><strong>形式2：typedef 返回类型(*新类型)(参数表)</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-keyword">typedef</span> <span class="hljs-title">char</span> <span class="hljs-params">(*PTRFUN)</span><span class="hljs-params">(<span class="hljs-type">int</span>)</span></span>; <br>PTRFUN pFun; <br><span class="hljs-function"><span class="hljs-type">char</span> <span class="hljs-title">glFun</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span>&#123; <span class="hljs-keyword">return</span>;&#125; <br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span><br><span class="hljs-function"></span>&#123; <br>    pFun = glFun; <br>    (*pFun)(<span class="hljs-number">2</span>); <br>&#125;<br><span class="hljs-comment">//typedef的作用：将声明变量得到的结果（变量名）提升为类型,也就是说pfunc现在是一种和函数指针类型等价的类型。</span><br></code></pre></td></tr></table></figure><h3 id="union-共用体">union 共用体</h3><p>事实上我们用到共用体并不多，一个常见的实例是形容某学校的老师和学生，学生存储分数而老师存储课程名字<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">struct</span> &#123;<br>    <span class="hljs-type">char</span> name[<span class="hljs-number">20</span>];<br>    <span class="hljs-type">int</span> num;<br>    <span class="hljs-type">char</span> sex;<br>    <span class="hljs-type">char</span> profession;<br>    <span class="hljs-keyword">union</span>&#123;<br>        <span class="hljs-type">float</span> score;<br>        <span class="hljs-type">char</span> course[<span class="hljs-number">20</span>];<br>    &#125; sc;<br>&#125; bodys[TOTAL];<br></code></pre></td></tr></table></figure> 要理解可以测试以下代码 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">union</span> <span class="hljs-title class_">content</span>&#123;<br>  <span class="hljs-type">int</span> num;<br>  <span class="hljs-type">char</span> ch;<br>  <span class="hljs-type">double</span> dbnum;<br>&#125;;<br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">point</span><br>&#123; <span class="hljs-type">int</span> x;<br>  <span class="hljs-type">int</span> y;<br>  <span class="hljs-type">int</span> z;&#125;;<br><span class="hljs-keyword">union</span> <span class="hljs-title class_">gaze</span><br>&#123;<br>    <span class="hljs-keyword">struct</span> <span class="hljs-title class_">point</span> p;<br>    <span class="hljs-type">int</span> data ;<br>&#125;;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">union</span> <span class="hljs-title class_">content</span> text ;<br>    std::cout&lt;&lt;<span class="hljs-string">&quot;sizeof(text)&quot;</span>&lt;&lt;<span class="hljs-built_in">sizeof</span>(text)&lt;&lt;std::endl;<br>    text.num=<span class="hljs-number">3</span>;<br>    text.ch =<span class="hljs-string">&#x27;g&#x27;</span>;<br>    text.dbnum = <span class="hljs-number">6</span>;<span class="hljs-comment">//可在这三步设置断点查看各值</span><br>    std::cout&lt;&lt;<span class="hljs-string">&quot;text.num= &quot;</span>&lt;&lt;text.num&lt;&lt;std::endl;<br>   <span class="hljs-comment">//条件编译指令</span><br>    <span class="hljs-meta">#<span class="hljs-keyword">if</span> 0</span><br>    <span class="hljs-keyword">union</span> <span class="hljs-title class_">gaze</span> a;<br>    a.p.x=<span class="hljs-number">3</span>;<br>    a.p.y=<span class="hljs-number">4</span>;<br>    a.p.z=<span class="hljs-number">5</span>;<br>    a.data =<span class="hljs-number">100</span>;<br>    std::cout&lt;&lt;<span class="hljs-string">&quot;a.p.z=&quot;</span>&lt;&lt;a.p.y&lt;&lt;<span class="hljs-string">&quot;\t a.data=&quot;</span>&lt;&lt;a.data&lt;&lt;std::endl;<br>    <span class="hljs-meta">#<span class="hljs-keyword">endif</span></span><br>&#125;<br></code></pre></td></tr></table></figure> ####union和struct内存占用</p><p>区别于struct,虽然语法基本一致，但是结构体和共用体的区别在于：结构体的各个成员会占用不同的内存，互相之间没有影响；而共用体的所有成员占用同一段内存，修改一个成员会影响其余所有成员。</p><p>结构体占用的内存大于等于所有成员占用的内存的总和（成员之间可能会存在缝隙）采用内存对齐模式，共用体占用的内存等于最长的成员占用的内存。共用体使用了内存覆盖技术，同一时刻只能保存一个成员的值，如果对新的成员赋值，就会把原来成员的值覆盖掉。故其占用内存为共用数据类型中最大的那个。</p><blockquote><p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">struct</span> <span class="hljs-title class_">HF</span>&#123;<span class="hljs-comment">// memory For struct </span><br> <span class="hljs-type">int</span> data;<br> <span class="hljs-type">char</span>* name;<br> <span class="hljs-type">unsigned</span> <span class="hljs-type">char</span> rank;<br>&#125;;<br></code></pre></td></tr></table></figure></p></blockquote><blockquote><p>其中data和name都是 4 bytes而rank本应该是1bytes,但由于内存对齐，故<code>sizeof(HF)</code>的结果应该是12.</p></blockquote><blockquote><p>此外注意<code>char* name</code>为char型指针，占用内存和指向字符串长度无关。而<br><code>char[10] name</code>则<code>sizeof(name)=10</code></p><h3 id="const的用法">const的用法</h3><p>const名叫常量限定符，用来限定特定变量，以通知编译器该变量是不可修改的。习惯性的使用const，可以避免在函数中对某些不应修改的变量造成可能的改动。常类型的对象必须进行初始化，而且不能被更新。const的用法大致可分为以下几个方面：</p></blockquote><p>(1)const修饰基本数据类型</p><p>(2)const应用到函数中</p><p>(3)const在类中的用法</p><p>(4)const修饰类对象，定义常量对象</p><h4 id="const修饰基本数据类型">1.const修饰基本数据类型</h4><ul><li>1.const修饰一般常量及数组</li></ul><p><code>const int a=10; // 等价的书写方式：     int const a=10;</code><code>const int arr[3]=&#123;1,2,3&#125;;           //      int const arr[3]=&#123;1,2,3&#125;;</code></p><ul><li><p>2.const修饰指针变量*及引用变量&amp;</p><p>A: const 修饰指针指向的内容，则内容为不可变量。 B: const修饰指针，则指针为不可变量。 C: const修饰指针和指针指向的内容，则指针和指针指向的内容都为不可变量。 对于 A:<code>const int *p = 8;</code> 则指针指向的内容 8不可改变。简称左定值，因为 const 位于 * 号的左边。 对于 B: <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">int</span> a = <span class="hljs-number">8</span>;<br><span class="hljs-type">int</span>* <span class="hljs-type">const</span> p = &amp;a;<br>*p = <span class="hljs-number">9</span>; <span class="hljs-comment">// 正确</span><br><span class="hljs-type">int</span>  b = <span class="hljs-number">7</span>;<br>p = &amp;b; <span class="hljs-comment">// 错误</span><br></code></pre></td></tr></table></figure>对于 const 指针 p其指向的内存地址不能够被改变，但其内容可以改变。</p></li></ul><p>简称，右定向。因为 const 位于 * 号的右边。 对于 C: 则是 A 和 B的合并<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs angelscript"><span class="hljs-built_in">int</span> a = <span class="hljs-number">8</span>;<br><span class="hljs-keyword">const</span> <span class="hljs-built_in">int</span> * <span class="hljs-keyword">const</span>  p = &amp;a;<br></code></pre></td></tr></table></figure> 这时，const p的指向的内容和指向的内存地址都已固定，不可改变。</p><p>对于 A，B，C 三种情况，根据 const 位于 *号的位置不同，我总结三句话便于记忆的话："左定值，右定向，const修饰不变量"。- 关于引用 1、普通引用 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs cpp">① <span class="hljs-type">int</span> i = <span class="hljs-number">3</span>; <br>  <span class="hljs-type">int</span> &amp;ri = i;    <span class="hljs-comment">//正确，引用绑定到int 变量i上</span><br> <br>② <span class="hljs-type">double</span> d = <span class="hljs-number">3.1415</span>；<br>  <span class="hljs-type">int</span> &amp;rd = d;   <span class="hljs-comment">//错误， 引用类型为 int ，所绑定对象类型为 double，类型不一致</span><br> <br>③ <span class="hljs-type">int</span> &amp; rm = <span class="hljs-number">3</span>； <span class="hljs-comment">//错误， 普通引用必须绑定到对象，不能绑定至常量</span><br> <br><span class="hljs-number">2</span>、<span class="hljs-type">const</span>修饰引用<br><br>① <span class="hljs-type">const</span> <span class="hljs-type">int</span> &amp;ci = <span class="hljs-number">3</span>; <span class="hljs-comment">//正确，整型字面值常量绑定到 const引用</span><br><span class="hljs-comment">//c++编译器  会  分配内存空间  ,c++编译器把ci放在符号表中</span><br>   <span class="hljs-comment">//  int  temp  =  3 </span><br>   <span class="hljs-comment">//  const  int  &amp;ci  =  temp;</span><br>②<span class="hljs-number">1</span>） <span class="hljs-type">int</span> i = <span class="hljs-number">1</span>；<br>    <span class="hljs-type">const</span> <span class="hljs-type">int</span> &amp;cj = i;    <span class="hljs-comment">//正确，非常量对象绑定到 const引用</span><br>    <span class="hljs-comment">//cj=2;// 错误 cj被const修饰，不可以改变</span><br>    i=<span class="hljs-number">3</span>;<span class="hljs-comment">//正确，i可以被修改，cj也被更改，cout&lt;&lt;cj时为3</span><br>    <span class="hljs-type">int</span> &amp;cj2 = i; <br>    cj2=<span class="hljs-number">5</span>;<span class="hljs-comment">//正确，i被修改，cj也被更改，cout&lt;&lt;cj时为5</span><br> <br><span class="hljs-number">2</span>） <span class="hljs-type">double</span> i= <span class="hljs-number">1.2</span>;<br>    <span class="hljs-type">const</span> <span class="hljs-type">int</span> &amp;cj = i;    <span class="hljs-comment">//正确，非常量对象绑定到 const引用</span><br>    <span class="hljs-comment">//cj=2.3;// 错误 cj被const修饰，不可以改变</span><br>    i=<span class="hljs-number">3.3</span>;<span class="hljs-comment">//正确，i可以被修改，cj没有被更改，cout&lt;&lt;cj时为1</span><br>    <span class="hljs-type">double</span> &amp;cj2 = i; <br>    cj2=<span class="hljs-number">5.2</span>;<span class="hljs-comment">//正确，i被修改为5.2，cj2被更改,为5.2，cj没有被更改，cout&lt;&lt;cj时为1</span><br> <br>原因：引用变量类型为<span class="hljs-type">int</span>，被引用对象类型为<span class="hljs-type">double</span>。<br>在进行<span class="hljs-type">const</span> <span class="hljs-type">int</span> &amp;cj = i;前，进行了如下操作<br> <span class="hljs-type">double</span> i= <span class="hljs-number">1.2</span>;<br> <span class="hljs-type">int</span> temp = i;<br> <span class="hljs-type">const</span> <span class="hljs-type">int</span> &amp;cj = temp;<span class="hljs-comment">//所以cj并未真正绑定对象i</span><br> <br>③ <span class="hljs-type">const</span> <span class="hljs-type">int</span> i = <span class="hljs-number">4</span>； <br>  <span class="hljs-type">const</span> <span class="hljs-type">int</span> &amp;ck =  i; <span class="hljs-comment">//正确，常量对象绑定到 const引用</span><br> <br>④ <span class="hljs-type">const</span> <span class="hljs-type">int</span> i = <span class="hljs-number">5</span>；<br>    <span class="hljs-type">int</span> &amp;r = i;    <span class="hljs-comment">//错误，常量对象绑定到非const引用</span><br></code></pre></td></tr></table></figure> <strong>结论：</strong> const引用的目的是,禁止通过修改引用值来改变被引用的对象。</p><p>1）const int &amp; e 相当于 const int * const e</p><p>2）普通引用 相当于 int *const e</p><p>3）当使用常量（字面量）对const引用进行初始化时，C++编译器会为常量值分配空间，并将引用名作为这段空间的别名</p><p>4）使用字面量对const引用初始化后，将生成一个只读变量</p><h4 id="作为函数返回值的const修饰符">2.作为函数返回值的const修饰符</h4><p>其实，不论是参数还是返回值，道理都是一样的，参数传入时候和函数返回的时候，初始化const变量</p><ol type="1"><li>修饰参数的const，如 void fun0(const A* a ); void fun1(const A&amp;a);</li></ol><p>调用函数的时候，用相应的变量初始化const常量，则在函数体中，按照const所修饰的部分进行常量化，如形参为constA* a， 则不能对传递进来的指针的内容进行改变，保护了原指针所指向的内容；如形参为const A&amp;a，则不能对传递进来的引用对象进行改变，保护了原对象的属性。</p><p>[注意]：参数const通常用于参数为指针或引用的情况; 2.修饰返回值的const，如const A fun2( ); const A* fun3( );</p><p>这样声明了返回值后，const按照"修饰原则"进行修饰，起到相应的保护作用。</p><p>一般用const修饰返回值为对象本身（非引用和指针）的情况多用于二目操作符重载函数并产生新对象的时候</p><h4 id="类中常量const的特殊用法">3.类中常量（const的特殊用法）</h4><p>1.使用枚举类型</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">class</span> <span class="hljs-title class_">test</span><br>&#123;<br>     <span class="hljs-keyword">enum</span> &#123; SIZE1 = <span class="hljs-number">10</span>, SIZE2 = <span class="hljs-number">20</span>&#125;; <span class="hljs-comment">// 枚举常量</span><br>     <span class="hljs-type">int</span> array1[SIZE1];  <br>     <span class="hljs-type">int</span> array2[SIZE2];<br>&#125;;<br></code></pre></td></tr></table></figure><p>2.使用const 或staticC++11仅不允许在类声明中初始化static非const类型的数据成员。 <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// using c++11 standard</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CTest11</span><br>&#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-type">static</span> <span class="hljs-type">const</span> <span class="hljs-type">int</span> a = <span class="hljs-number">3</span>; <span class="hljs-comment">// Ok in C++11</span><br>    <span class="hljs-type">static</span> <span class="hljs-type">int</span> b = <span class="hljs-number">4</span>;       <span class="hljs-comment">// Error</span><br>    <span class="hljs-type">const</span> <span class="hljs-type">int</span> c = <span class="hljs-number">5</span>;        <span class="hljs-comment">// Ok in C++11</span><br>    <span class="hljs-type">int</span> d = <span class="hljs-number">6</span>;              <span class="hljs-comment">// Ok in C++11</span><br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">CTest11</span>() :<span class="hljs-built_in">c</span>(<span class="hljs-number">0</span>) &#123; &#125;     <span class="hljs-comment">// Ok in C++11</span><br>&#125;;<br> <br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;<br>    CTest11 testObj;<br>    cout &lt;&lt; testObj.a &lt;&lt; testObj.b &lt;&lt; testObj.c &lt;&lt; testObj.d &lt;&lt; endl;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span><span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Test</span><br>&#123;<br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-built_in">Test</span>(<span class="hljs-type">int</span> _m,<span class="hljs-type">int</span> <span class="hljs-type">_t</span>):_cm(_m),_ct(<span class="hljs-type">_t</span>)&#123;&#125;<br>    <span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">Kf</span><span class="hljs-params">()</span><span class="hljs-type">const</span></span><br><span class="hljs-function">    </span>&#123;<br>        ++_cm; <span class="hljs-comment">// 错误</span><br>        ++_ct; <span class="hljs-comment">// 正确</span><br>    &#125;<br><span class="hljs-keyword">private</span>:<br>    <span class="hljs-type">int</span> _cm;<br>    <span class="hljs-keyword">mutable</span> <span class="hljs-type">int</span> _ct;<br>&#125;;<br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-function">Test <span class="hljs-title">t</span><span class="hljs-params">(<span class="hljs-number">8</span>,<span class="hljs-number">7</span>)</span></span>;<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure></p><h3 id="有关动态分配内存与内存泄漏">有关动态分配内存与内存泄漏</h3><p>前提1.指定变量的地址，编译器在函数结束后会自动释放内存，如</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">func</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123;    <span class="hljs-type">int</span> a=<span class="hljs-number">100</span>; <span class="hljs-type">int</span>* p= &amp;a&#125;;<span class="hljs-comment">//分配在栈空间</span><br></code></pre></td></tr></table></figure><p>但是,动态申请内存(比如new或者malloc,realloc)</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">func</span><span class="hljs-params">()</span></span><br><span class="hljs-function"></span>&#123; <span class="hljs-type">int</span> *p = <span class="hljs-keyword">new</span> a[<span class="hljs-number">100</span>];<span class="hljs-comment">//分配在堆空间</span><br> <span class="hljs-keyword">delete</span>[] p ;<span class="hljs-comment">//记得申请了多位内存加上方括号</span><br>&#125;<br></code></pre></td></tr></table></figure><p>实际情况中，由于数据的传来传去，到何处释放内存就成了一个复杂的问题，就容易忘记释放内存。</p><p>*现代C++利用智能指针可以解决这类问题</p><p>C++11后nullptr代替NULL但比NULL更安全(有时NULL会被编译器误解成int型?)</p><h3 id="c-内联函数">C++ 内联函数</h3><p><strong>内联函数inline：</strong>引入内联函数的目的是为了解决程序中函数调用的效率问题</p><p>程序在编译器编译的时候，编译器将程序中出现的内联函数的调用表达式用内联函数的函数体进行替换，而对于其他的函数，都是在运行时候才被替代。这其实就是个空间代价换时间的节省。所以内联函数一般都是1-5行的小函数。在使用内联函数时要留神：</p><ul><li><p>1.在内联函数内不允许使用循环语句和开关语句；</p></li><li><p>2.内联函数的定义必须出现在内联函数第一次调用之前；</p></li><li><p>3.类结构中所在的类说明内部定义的函数是内联函数。</p></li></ul><p>有些函数即使声明为内联的也不一定会被编译器内联, 这点很重要;比如虚函数和递归函数就不会被正常内联.</p><p>通常,递归函数不应该声明成内联函数.(递归调用堆栈的展开并不像循环那么简单,比如递归层数在编译时可能是未知的, 大多数编译器都不支持内联递归函数).</p><p>虚函数内联的主要原因则是想把它的函数体放在类定义内, 为了图个方便,抑或是当作文档描述其行为, 比如精短的存取函数.</p>]]></content>
    
    
    <categories>
      
      <category>CPP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>const</tag>
      
      <tag>内存</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>vscode上实现cmake工程下的调试配置</title>
    <link href="/2023/01/22/Code/env/vscode_config/"/>
    <url>/2023/01/22/Code/env/vscode_config/</url>
    
    <content type="html"><![CDATA[<h1id="在vscode上实现多文件cmake工程下的调试">在vscode上实现多文件cmake工程下的调试</h1><p><strong>环境: ubuntu-VMware (中科大-vlab)</strong></p><p>文件结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">├──.vscode<br>│   ├── tasks.json<br>│   ├── launch.json<br>├── build<br>..<br>├── CMakeLists.txt<br>├── include<br>│   ├── Gun.h<br>│   └── Soider.h<br>├── main.cpp<br>└── src<br>    ├── Gun.cpp<br>    └── Soider.cpp<br></code></pre></td></tr></table></figure><p>如果是利用g++构建，command应该为</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd build<br>cmake ..<br>make<br></code></pre></td></tr></table></figure><p>然后调用gdb进行调试(但是命令行gdb感觉还是不太习惯，于是使用vscode)</p><blockquote><p>本文参考了网上一些blog,仅用于学习使用</p><p>本文只是讲解如何读懂和修改两个.json文件仅用于帮助我们工程的运行与调试，并不着力.json的开发，不会讨论其背后的原理。</p><p>下面只是讨论一个例子(实验)，并不完全适用于模板</p></blockquote><h2 id="task环境配置">task环境配置</h2><p>具体我是看的这个中文版教程<ahref="https://geek-docs.com/vscode/vscode-tutorials/vscode-task-system-configuration-grouping-and-result-display.html">VSCode任务系统配置 – 分组和结果显示|极客教程(geek-docs.com)</a>，但是一般可以套用模板，只要实际应用中改相应几个参数就行。</p><p><strong>第一个属性是 label标签</strong>，就是这个任务的名字。我们在命令面板里执行任务会需要读到它，所以它的值应该尽可能地描述这个任务是干什么的。</p><p><strong>第二个属性是 type类型</strong>。对于自定义的任务来说，这个类型可以有两种选择，一种是这个任务被当作进程来运行，另一种则是在shell 中作为命令行来运行。默认情况下我们会在 shell 下运行，而且这个shell 命令行将会在集成终端里执行，shell的选择则会尊重我们在集成终端的配置。比如在我的例子中，系统默认的 shell是 zsh，那么当我运行这个脚本时，就会在 zsh 里执行。</p><p><strong>第三个属性是命令command</strong>(和args)，它代表着我们希望在shell 中运行哪一个命令，或者我们希望运行哪个程序。</p><blockquote><p>对于不同的操作系统，可以使用特定的标签</p><p>比如我们可以为 Windows 或者 Linux 系统指定特定的地址，</p></blockquote><blockquote><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;tasks&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-comment">// 任务一： 创建 build 文件夹</span><br>            <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;shell&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;CreateBuildDir&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-comment">// lable 标记任务名称</span><br>            <span class="hljs-attr">&quot;command&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;mkdir&quot;</span><span class="hljs-punctuation">,</span>  <span class="hljs-comment">// 命令</span><br>            <span class="hljs-comment">// 传给上面命令的参数，这里是传给 Unix 系统的参数，windows下稍有不用，下边有</span><br>            <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>                <span class="hljs-string">&quot;-p&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-string">&quot;build&quot;</span><br>            <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <br>            <span class="hljs-attr">&quot;windows&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;options&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                    <span class="hljs-attr">&quot;shell&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                        <span class="hljs-attr">&quot;executable&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;powershell.exe&quot;</span><br>                    <span class="hljs-punctuation">&#125;</span><br>                <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>   <span class="hljs-comment">// 对于windows系统，传的参数</span><br>                    <span class="hljs-string">&quot;-Force&quot;</span><span class="hljs-punctuation">,</span><br>                    <span class="hljs-string">&quot;build&quot;</span><br>                <span class="hljs-punctuation">]</span><br>            <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;options&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;cwd&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;problemMatcher&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>                <span class="hljs-string">&quot;$gcc&quot;</span><br>            <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-comment">// 任务二： Cmake</span><br>        <span class="hljs-comment">// 在 build 文件夹中调用 cmake 进行项目配置</span><br>        <span class="hljs-comment">// 如果想配置比如 release 还是 debug 可以添加参数或者在</span><br>        <span class="hljs-comment">// CMakeLists.txt中设置也行</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;shell&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;cmakeRun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-comment">// 给这个任务起个名字</span><br><span class="hljs-comment">// 这里的cmake，用我后面小程序创建的结果填的是全路径，</span><br><span class="hljs-comment">// 命令写全路径，则路径中不能包含带空格</span><br><span class="hljs-comment">// 如果你添加了环境变量，那么直接填写命令即可，也不会有</span><br><span class="hljs-comment">// 路径是否包含空格的问题（下面的命令同理）</span><br>            <span class="hljs-attr">&quot;command&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;cmake&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>            <span class="hljs-string">&quot;-DCMAKE_MAKE_PROGRAM=E:\\Resource\\mingw64\\bin\\mingw32-make.exe&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-comment">// MinGW目录下bin目录下的mingw32-make.exe</span><br>                <span class="hljs-string">&quot;-G&quot;</span><span class="hljs-punctuation">,</span><br>                <span class="hljs-comment">// 不使用-G &quot;Unix Makefiles&quot; 参数可能会编译成了VS用的工程文件</span><br>                <span class="hljs-comment">// 之所以三个斜杠，是因为vscode终端自己还要转义一次</span><br>                <span class="hljs-comment">// 2021-01-21更新：我在32位的win7上发现，vscode自己又不转义了</span><br>                <span class="hljs-comment">// 所以如果以下三个斜杠不行的话，大家手动改成一个斜杠就好，即\&quot;Unix Makefiles\&quot;</span><br>                <span class="hljs-comment">// 后面我给的小程序默认写的是3个</span><br>                <span class="hljs-string">&quot;\\\&quot;Unix Makefiles\\\&quot;&quot;</span><span class="hljs-punctuation">,</span>  <br>                <span class="hljs-string">&quot;../&quot;</span>  <span class="hljs-comment">// ../ 表示build文件夹的上级目录，CMakeLists.txt就放在上级目录中</span><br>            <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;options&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;cwd&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;/build&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;dependsOn&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-string">&quot;CreateBuildDir&quot;</span>  <span class="hljs-comment">// 表示在 创建目录 任务结束后进行</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-comment">// 任务三： make编译</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;shell&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;label&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;makeRun&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;command&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;mingw32-make&quot;</span><span class="hljs-punctuation">,</span>  <span class="hljs-comment">// 这个也是MinGW目录下bin目录下的mingw32-make.exe，如果添加了环境变量，这里直接写mingw32-make.exe</span><br>            <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;options&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">&#123;</span><br>                <span class="hljs-attr">&quot;cwd&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;/build&quot;</span><br>            <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span> <span class="hljs-comment">// 注意这里是编译到了项目文件夹下的 build 文件夹里面，这里就解释了</span><br>            <span class="hljs-comment">// 为什么 launch.json 中 program 路径要那么设置了。</span><br>            <span class="hljs-attr">&quot;dependsOn&quot;</span><span class="hljs-punctuation">:</span><span class="hljs-punctuation">[</span><br>                <span class="hljs-string">&quot;cmakeRun&quot;</span>  <span class="hljs-comment">// 表示在Cmake任务结束后进行</span><br>            <span class="hljs-punctuation">]</span><br>        <span class="hljs-punctuation">&#125;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;2.0.0&quot;</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure></blockquote><p>于是在我这个样例中可以这样写:</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs js">&#123;   <br>    <span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;2.0.0&quot;</span>,<br>    <span class="hljs-string">&quot;options&quot;</span>: &#123;<br>        <span class="hljs-string">&quot;cwd&quot;</span>: <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;/build&quot;</span> <span class="hljs-comment">//注意提前创建</span><br>    &#125;,<br>    <span class="hljs-string">&quot;tasks&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;shell&quot;</span>,<br>            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cmake&quot;</span>,<br>            <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;cmake&quot;</span>,      <span class="hljs-comment">//这里根据你具体工程源文件设置</span><br>            <span class="hljs-string">&quot;args&quot;</span>: [<br>                <span class="hljs-string">&quot;..&quot;</span><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;make&quot;</span>,        <span class="hljs-comment">//</span><br>            <span class="hljs-string">&quot;group&quot;</span>: &#123;<br>                <span class="hljs-string">&quot;kind&quot;</span>: <span class="hljs-string">&quot;build&quot;</span>,<br>                <span class="hljs-string">&quot;isDefault&quot;</span>: <span class="hljs-literal">true</span><br>            &#125;,<br>            <span class="hljs-string">&quot;command&quot;</span>: <span class="hljs-string">&quot;make&quot;</span>,<br>            <span class="hljs-string">&quot;args&quot;</span>: [<br><br>            ]<br>        &#125;,<br>        &#123;<br>            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Build&quot;</span>,<br><span class="hljs-string">&quot;dependsOrder&quot;</span>: <span class="hljs-string">&quot;sequence&quot;</span>, <span class="hljs-comment">// 按列出的顺序执行任务依赖项</span><br>            <span class="hljs-string">&quot;dependsOn&quot;</span>:[<br>                <span class="hljs-string">&quot;cmake&quot;</span>,<br>                <span class="hljs-string">&quot;make&quot;</span><br>            ]<br>        &#125;<br>    ]<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="launch环境配置">launch环境配置</h2><p>当我们按下按钮后，VS Code询问我们想要创建什么项目的调试配置，这里我们再次选择Node.js。然后我们就能够看到 .vscode 文件夹下 launch.json文件被创建出来了，它的内容如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br> <span class="hljs-comment">// 使用 IntelliSense 了解相关属性。 </span><br> <span class="hljs-comment">// 悬停以查看现有属性的描述。</span><br> <span class="hljs-comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span><br> <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;0.2.0&quot;</span><span class="hljs-punctuation">,</span><br> <span class="hljs-attr">&quot;configurations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>  <span class="hljs-punctuation">&#123;</span><br>   <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;node&quot;</span><span class="hljs-punctuation">,</span><br>   <span class="hljs-attr">&quot;request&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;launch&quot;</span><span class="hljs-punctuation">,</span><br>   <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;启动程序&quot;</span><span class="hljs-punctuation">,</span><br>   <span class="hljs-attr">&quot;program&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;file&#125;&quot;</span><br>  <span class="hljs-punctuation">&#125;</span><br> <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>这个 JSON 文件里的 configurations的值就是当前文件夹下所有的配置了。现在我们只有一个调试配置，它有四个属性：</p><p><strong>第一个是 type</strong>，代表着调试器的类型。它决定了 VS Code会使用哪个调试插件来调试代码。</p><p><strong>第二个是request</strong>，代表着该如何启动调试器。如果我们的代码已经运行起来了，则可以将它的值设为attach，那么我们则是使用调试器来调试这个已有的代码进程；而如果它的值是launch，则意味着我们会使用调试器直接启动代码并且调试。</p><p><strong>第三个属性 name</strong>，就是这个配置的名字了。</p><p><strong>第四个属性 program</strong>，就是告诉 Node.js调试器，我们想要调试哪个文件。这个值支持预定义参数，比如在上面的例子里，我们使用了${file}，也就是当前编辑器里打开的文件。</p><p>不过使用这个配置，并没有解决刚才上面我提的问题，如果所有文件都被关闭了，那么${file}就是空的了，这个调试配置并不能正确运行。</p><p>下面我们把 program 的值改为<code>$&#123;workspaceFolder&#125;/index.js</code>，其中<code>$&#123;workspaceFolder&#125;</code>是代表当前工作区文件夹地址的预定义参数，使用它就能够准确地定位当前工作区里<code>index.js</code>文件了。（关于在配置文件里可以使用的预定义参数，请参考<ahref="https://code.visualstudio.com/docs/editor/variables-reference">VisualStudio Code Variables Reference</a>。 ）</p><p>windows端配置如下，(linux感觉是下面删减去不必要的东西,地址记得吧//换成/)</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs js">&#123;<br>    <span class="hljs-comment">// 使用 IntelliSense 了解相关属性。 </span><br>    <span class="hljs-comment">// 悬停以查看现有属性的描述。</span><br>    <span class="hljs-comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span><br>    <span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.2.0&quot;</span>,<br>    <span class="hljs-string">&quot;configurations&quot;</span>: [<br>        &#123;<br>            <span class="hljs-string">&quot;name&quot;</span>: <span class="hljs-string">&quot;g++.exe - 生成和调试活动文件&quot;</span>,<br>             <span class="hljs-comment">// type 告诉vscode编译器的类型，我用的MinGW64也就是g++，这里是cppdgb</span><br>             <span class="hljs-comment">// 这个是规定的，不是随便写，比如msvc编译器就是cppvsdbg</span><br>            <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;cppdbg&quot;</span>, <br>            <span class="hljs-comment">//这个这里只能是launch</span><br>            <span class="hljs-string">&quot;request&quot;</span>: <span class="hljs-string">&quot;launch&quot;</span>,<br>            <span class="hljs-comment">// program 这个是你的可执行程序位置，这里可以根据自己的tasks.json生成</span><br>            <span class="hljs-comment">// 程序的位置自定义修改，等会参照后面的tasks.json内容</span><br>            <span class="hljs-string">&quot;program&quot;</span>: <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;\\build\\$&#123;workspaceRootFolderName&#125;.exe&quot;</span>,<br>            <span class="hljs-comment">// $&#123;xxxx&#125;是vscode内置的变量，可以方便获取到需要的路径或者文件名,</span><br>            <span class="hljs-comment">// $&#123;workspaceFolder&#125; :表示当前workspace文件夹路径，也即/home/Coding/Test</span><br><span class="hljs-comment">// $&#123;workspaceRootFolderName&#125;:表示workspace的文件夹名，也即Test</span><br><span class="hljs-comment">// $&#123;file&#125;:文件自身的绝对路径，也即/home/Coding/Test/.vscode/tasks.json</span><br><span class="hljs-comment">// $&#123;relativeFile&#125;:文件在workspace中的路径，也即.vscode/tasks.json</span><br><span class="hljs-comment">// $&#123;fileBasenameNoExtension&#125;:当前文件的文件名，不带后缀，也即tasks</span><br><span class="hljs-comment">// $&#123;fileBasename&#125;:当前文件的文件名，tasks.json</span><br><span class="hljs-comment">// $&#123;fileDirname&#125;:文件所在的文件夹路径，也即/home/Coding/Test/.vscode</span><br><span class="hljs-comment">// $&#123;fileExtname&#125;:当前文件的后缀，也即.json</span><br><span class="hljs-comment">// $&#123;lineNumber&#125;:当前文件光标所在的行号</span><br><span class="hljs-comment">// $&#123;env:PATH&#125;:系统中的环境变量</span><br>            <span class="hljs-string">&quot;args&quot;</span>: [], <br>            <span class="hljs-string">&quot;stopAtEntry&quot;</span>: <span class="hljs-literal">false</span>,<br>            <span class="hljs-string">&quot;cwd&quot;</span>: <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;&quot;</span>,<br>            <span class="hljs-string">&quot;environment&quot;</span>: [],<br>            <span class="hljs-string">&quot;externalConsole&quot;</span>: <span class="hljs-literal">true</span>,<br>            <span class="hljs-string">&quot;MIMode&quot;</span>: <span class="hljs-string">&quot;gdb&quot;</span>,<br>            <span class="hljs-comment">// 调试器的路径</span><br>            <span class="hljs-string">&quot;miDebuggerPath&quot;</span>: <span class="hljs-string">&quot;D:\\program\\mingw64\\bin\\gdb.exe&quot;</span>,<br>            <span class="hljs-string">&quot;setupCommands&quot;</span>: [<br>                &#123;<br>                    <span class="hljs-string">&quot;description&quot;</span>: <span class="hljs-string">&quot;为 gdb 启用整齐打印&quot;</span>,<br>                    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;-enable-pretty-printing&quot;</span>,<br>                    <span class="hljs-string">&quot;ignoreFailures&quot;</span>: <span class="hljs-literal">true</span><br>                &#125;<br>            ],<br>            <span class="hljs-comment">// preLaunchTask 表示在 执行调试前 要完成的任务</span><br>            <span class="hljs-comment">// 比如这里 要完成 makeRun 这个tasks任务（重新生成程序）</span><br>            <span class="hljs-comment">// 这里的 makeRun 是 tasks.json 中 lable 标记的任务名称</span><br>            <span class="hljs-string">&quot;preLaunchTask&quot;</span>: <span class="hljs-string">&quot;makeRun&quot;</span>,<br>        &#125;<br>    ]<br>&#125;<br><span class="hljs-comment">// 上面没有注释的部分基本都是默认生成的，可以不用更改的部分</span><br></code></pre></td></tr></table></figure><p>在这个例子中，我们这样写:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-comment">// 使用 IntelliSense 了解相关属性。 </span><br>    <span class="hljs-comment">// 悬停以查看现有属性的描述。</span><br>    <span class="hljs-comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span><br>    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;0.2.0&quot;</span><span class="hljs-punctuation">,</span><br>    <span class="hljs-attr">&quot;configurations&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>        <span class="hljs-punctuation">&#123;</span><br>            <span class="hljs-attr">&quot;name&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;g++.exe - 生成和调试活动文件&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;type&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;cppdbg&quot;</span><span class="hljs-punctuation">,</span> <br>            <span class="hljs-attr">&quot;request&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;launch&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;program&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;/build/my_cmake&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;args&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <br>            <span class="hljs-attr">&quot;stopAtEntry&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">false</span></span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;cwd&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;$&#123;workspaceFolder&#125;&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;environment&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;externalConsole&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;MIMode&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;gdb&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;miDebuggerPath&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;/usr/bin/gdb&quot;</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;setupCommands&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><br>                <span class="hljs-punctuation">&#123;</span><br>                    <span class="hljs-attr">&quot;description&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;为 gdb 启用整齐打印&quot;</span><span class="hljs-punctuation">,</span><br>                    <span class="hljs-attr">&quot;text&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;-enable-pretty-printing&quot;</span><span class="hljs-punctuation">,</span><br>                    <span class="hljs-attr">&quot;ignoreFailures&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><br>                <span class="hljs-punctuation">&#125;</span><br>            <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br>            <span class="hljs-attr">&quot;preLaunchTask&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Build&quot;</span><span class="hljs-punctuation">,</span><br>        <span class="hljs-punctuation">&#125;</span><br>    <span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>环境</category>
      
    </categories>
    
    
    <tags>
      
      <tag>VSCode</tag>
      
      <tag>json</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Test</title>
    <link href="/2023/01/22/firstblog/"/>
    <url>/2023/01/22/firstblog/</url>
    
    <content type="html"><![CDATA[<h1 id="first-blog">First Blog</h1><p>As you can see ,这是一个基于Hexo的Blog //测试code: <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br>A = np.array([[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]])<br></code></pre></td></tr></table></figure> <imgsrc="/img/logo.PNG" alt="logo" /> //测试LATEX</p><p><span class="math display">\[x=\frac12\mu\]</span></p>]]></content>
    
    
    <categories>
      
      <category>Code</category>
      
    </categories>
    
    
    <tags>
      
      <tag>test</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/01/20/hello-world/"/>
    <url>/2023/01/20/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a>## for more 感觉还不错 have a try</p>]]></content>
    
    
    <categories>
      
      <category>Help</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
